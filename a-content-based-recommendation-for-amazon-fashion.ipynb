{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Bases Recommdataion Using Amazon Fashion Dataset\n",
    "- Problem Statemnet: \"Recommend an Product to user based on Similarity of item/Content/Product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:12.650950Z",
     "iopub.status.busy": "2023-12-09T04:16:12.650434Z",
     "iopub.status.idle": "2023-12-09T04:16:16.331081Z",
     "shell.execute_reply": "2023-12-09T04:16:16.329992Z",
     "shell.execute_reply.started": "2023-12-09T04:16:12.650908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "directory = \"/kaggle/input/amazon-fashion-products-2020/marketing_sample_for_amazon_com-amazon_fashion_products__20200201_20200430__30k_data.ldjson\"\n",
    "data=pd.read_json(directory, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.333444Z",
     "iopub.status.busy": "2023-12-09T04:16:16.333130Z",
     "iopub.status.idle": "2023-12-09T04:16:16.339864Z",
     "shell.execute_reply": "2023-12-09T04:16:16.338760Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.333417Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data point:  30000 \n",
      "Number of feature : 33\n"
     ]
    }
   ],
   "source": [
    "# print the number of feature and datapoint Dataset have?\n",
    "print(\"Number of data point: \", data.shape[0],\"\\nNumber of feature :\",data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.342052Z",
     "iopub.status.busy": "2023-12-09T04:16:16.341619Z",
     "iopub.status.idle": "2023-12-09T04:16:16.353029Z",
     "shell.execute_reply": "2023-12-09T04:16:16.351921Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.342013Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uniq_id', 'crawl_timestamp', 'asin', 'product_url', 'product_name',\n",
      "       'image_urls__small', 'medium', 'large', 'browsenode', 'brand',\n",
      "       'sales_price', 'weight', 'rating', 'sales_rank_in_parent_category',\n",
      "       'sales_rank_in_child_category', 'delivery_type', 'meta_keywords',\n",
      "       'amazon_prime__y_or_n', 'parent___child_category__all',\n",
      "       'best_seller_tag__y_or_n', 'other_items_customers_buy',\n",
      "       'product_details__k_v_pairs', 'discount_percentage', 'colour',\n",
      "       'no__of_reviews', 'seller_name', 'seller_id', 'left_in_stock',\n",
      "       'no__of_offers', 'no__of_sellers', 'technical_details__k_v_pairs',\n",
      "       'formats___editions', 'name_of_author_for_books'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Features in Dataset\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of dataset\n",
    "- Total Records Count : 132685 \n",
    "- Date Range : 01st Feb 2020 - 30th Apr 2020  \n",
    "\n",
    "#### Available Fields:\n",
    "- uniq_id-- The unique ID of the product\n",
    "- crawl_timestamp-- The time of the crawl to pull the data\n",
    "- asin-- The ASIN of the product\n",
    "- product_url-- The URL of the product\n",
    "- product_name-- The name of the product\n",
    "- image_urls__small-- The url of the images in small size\n",
    "- medium-- The medium by which the product was seen\n",
    "- large-- the size of the file\n",
    "- browsenode\n",
    "- seller_name-- the name of the seller of the product\n",
    "- seller_id-- the ID of the seller of the product\n",
    "- brand-- the brand of the product\n",
    "- sales_price-- the price of the sale of the product\n",
    "- discount_percentage-- the discount that was being offered on the product\n",
    "- weight-- the weight of the product\n",
    "- rating-- the rating of the product\n",
    "- no__of_reviews-- The number of reviews that have been given to the product\n",
    "- delivery_type-- the type of delivery the product will be delivered to the buyer\n",
    "- meta_keywords-- the keywords used to search for the product\n",
    "- amazon_prime__y_or_n-- If the buyer has an amazon prime membership or no\n",
    "- best_seller_tag__y_or_n-- the tag of bestseller or no\n",
    "- technical_details__k_v_pairs-- the repair was given or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.354485Z",
     "iopub.status.busy": "2023-12-09T04:16:16.354123Z",
     "iopub.status.idle": "2023-12-09T04:16:16.441140Z",
     "shell.execute_reply": "2023-12-09T04:16:16.439948Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.354424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# select only the few feature from the dataset to perfom similartity/ content based recommendation \n",
    "data=data[['asin','product_url', 'product_name','sales_price','rating','meta_keywords','medium','brand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.444658Z",
     "iopub.status.busy": "2023-12-09T04:16:16.444312Z",
     "iopub.status.idle": "2023-12-09T04:16:16.452443Z",
     "shell.execute_reply": "2023-12-09T04:16:16.451307Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.444628Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asin', 'product_url', 'product_name', 'sales_price', 'rating',\n",
       "       'meta_keywords', 'medium', 'brand'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the Columns Using For this task\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- asin-- The ASIN of the product\n",
    "- product_url-- The URL of the product\n",
    "- product_name-- The name of the product\n",
    "- medium-- The medium by which the product was seen\n",
    "- brand-- the brand of the product\n",
    "- sales_price-- the price of the sale of the product\n",
    "- rating-- the rating of the product\n",
    "- meta_keywords-- the keywords used to search for the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.454171Z",
     "iopub.status.busy": "2023-12-09T04:16:16.453872Z",
     "iopub.status.idle": "2023-12-09T04:16:16.478502Z",
     "shell.execute_reply": "2023-12-09T04:16:16.477568Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.454143Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_name</th>\n",
       "      <th>sales_price</th>\n",
       "      <th>rating</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>medium</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B07STS2W9T</td>\n",
       "      <td>https://www.amazon.in/Facon-Kalamkari-Handbloc...</td>\n",
       "      <td>LA' Facon Cotton Kalamkari Handblock Saree Blo...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LA' Facon Cotton Kalamkari Handblock Saree Blo...</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>LA' Facon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B07N6TD2WL</td>\n",
       "      <td>https://www.amazon.in/Sf-Jeans-Pantaloons-T-Sh...</td>\n",
       "      <td>Sf Jeans By Pantaloons Men's Plain Slim fit T-...</td>\n",
       "      <td>265.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Sf Jeans By Pantaloons Men's Plain Slim fit T-...</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07WJ6WPN1</td>\n",
       "      <td>https://www.amazon.in/LOVISTA-Traditional-Prin...</td>\n",
       "      <td>LOVISTA Cotton Gota Patti Tassel Traditional P...</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>LOVISTA Cotton Gota Patti Tassel Traditional P...</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>LOVISTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07PYSF4WZ</td>\n",
       "      <td>https://www.amazon.in/People-Printed-Regular-T...</td>\n",
       "      <td>People Men's Printed Regular fit T-Shirt</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>People Men's Printed Regular fit T-Shirt,People</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B082KXNM7X</td>\n",
       "      <td>https://www.amazon.in/Monte-Carlo-Cotton-Colla...</td>\n",
       "      <td>Monte Carlo Grey Solid Cotton Blend Polo Colla...</td>\n",
       "      <td>1914.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Monte Carlo Grey Solid Cotton Blend Polo Colla...</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                        product_url  \\\n",
       "0  B07STS2W9T  https://www.amazon.in/Facon-Kalamkari-Handbloc...   \n",
       "1  B07N6TD2WL  https://www.amazon.in/Sf-Jeans-Pantaloons-T-Sh...   \n",
       "2  B07WJ6WPN1  https://www.amazon.in/LOVISTA-Traditional-Prin...   \n",
       "3  B07PYSF4WZ  https://www.amazon.in/People-Printed-Regular-T...   \n",
       "4  B082KXNM7X  https://www.amazon.in/Monte-Carlo-Cotton-Colla...   \n",
       "\n",
       "                                        product_name  sales_price  rating  \\\n",
       "0  LA' Facon Cotton Kalamkari Handblock Saree Blo...        200.0     5.0   \n",
       "1  Sf Jeans By Pantaloons Men's Plain Slim fit T-...        265.0     3.6   \n",
       "2  LOVISTA Cotton Gota Patti Tassel Traditional P...        660.0     3.5   \n",
       "3           People Men's Printed Regular fit T-Shirt        195.0     3.0   \n",
       "4  Monte Carlo Grey Solid Cotton Blend Polo Colla...       1914.0     5.0   \n",
       "\n",
       "                                       meta_keywords  \\\n",
       "0  LA' Facon Cotton Kalamkari Handblock Saree Blo...   \n",
       "1  Sf Jeans By Pantaloons Men's Plain Slim fit T-...   \n",
       "2  LOVISTA Cotton Gota Patti Tassel Traditional P...   \n",
       "3    People Men's Printed Regular fit T-Shirt,People   \n",
       "4  Monte Carlo Grey Solid Cotton Blend Polo Colla...   \n",
       "\n",
       "                                              medium      brand  \n",
       "0  https://images-na.ssl-images-amazon.com/images...  LA' Facon  \n",
       "1  https://images-na.ssl-images-amazon.com/images...        NaN  \n",
       "2  https://images-na.ssl-images-amazon.com/images...    LOVISTA  \n",
       "3  https://images-na.ssl-images-amazon.com/images...        NaN  \n",
       "4  https://images-na.ssl-images-amazon.com/images...        NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see few instance of dataset \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.480197Z",
     "iopub.status.busy": "2023-12-09T04:16:16.479906Z",
     "iopub.status.idle": "2023-12-09T04:16:16.524770Z",
     "shell.execute_reply": "2023-12-09T04:16:16.523599Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.480171Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   asin           30000 non-null  object \n",
      " 1   product_url    30000 non-null  object \n",
      " 2   product_name   30000 non-null  object \n",
      " 3   sales_price    27110 non-null  float64\n",
      " 4   rating         30000 non-null  float64\n",
      " 5   meta_keywords  30000 non-null  object \n",
      " 6   medium         29998 non-null  object \n",
      " 7   brand          21857 non-null  object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Imformation Of Dataset \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal With nullvalues in Differnt Feaature\n",
    "- As we can See out of 30000 product \n",
    "    - 27110 sales price are only non null\n",
    "    - 21857 brand have null values\n",
    "    - only for two product we don't have any image\n",
    "- Remove the Rows having null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.526125Z",
     "iopub.status.busy": "2023-12-09T04:16:16.525840Z",
     "iopub.status.idle": "2023-12-09T04:16:16.582877Z",
     "shell.execute_reply": "2023-12-09T04:16:16.581584Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.526100Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19233 entries, 0 to 29999\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   asin           19233 non-null  object \n",
      " 1   product_url    19233 non-null  object \n",
      " 2   product_name   19233 non-null  object \n",
      " 3   sales_price    19233 non-null  float64\n",
      " 4   rating         19233 non-null  float64\n",
      " 5   meta_keywords  19233 non-null  object \n",
      " 6   medium         19233 non-null  object \n",
      " 7   brand          19233 non-null  object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# remove the rows having null values\n",
    "data.dropna(axis=0, inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.584800Z",
     "iopub.status.busy": "2023-12-09T04:16:16.584284Z",
     "iopub.status.idle": "2023-12-09T04:16:16.606333Z",
     "shell.execute_reply": "2023-12-09T04:16:16.604951Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.584765Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     19233\n",
      "unique     5403\n",
      "top         Max\n",
      "freq        500\n",
      "Name: brand, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Understand the feature \"Brand\"\n",
    "print(data['brand'].describe())\n",
    "\n",
    "# Max is an Top most frequent brand and it have around 5403 brands in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.608290Z",
     "iopub.status.busy": "2023-12-09T04:16:16.607985Z",
     "iopub.status.idle": "2023-12-09T04:16:16.627662Z",
     "shell.execute_reply": "2023-12-09T04:16:16.626270Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.608263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Max', 500),\n",
       " ('Generic', 223),\n",
       " ('BIBA', 205),\n",
       " ('Mothercare', 156),\n",
       " ('Campus Sutra', 150),\n",
       " ('Soch', 149),\n",
       " ('nauti nati', 132),\n",
       " ('Ada', 125),\n",
       " ('GRITSTONES', 110),\n",
       " ('PrintOctopus', 102)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the Top 10 most frequent brand\n",
    "from collections import Counter\n",
    "counts=Counter(list(data['brand']))\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.630031Z",
     "iopub.status.busy": "2023-12-09T04:16:16.629588Z",
     "iopub.status.idle": "2023-12-09T04:16:16.650216Z",
     "shell.execute_reply": "2023-12-09T04:16:16.648896Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.629990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    19233.000000\n",
      "mean       902.009351\n",
      "std       1072.390707\n",
      "min         39.000000\n",
      "25%        365.000000\n",
      "50%        559.000000\n",
      "75%        899.000000\n",
      "max       9988.000000\n",
      "Name: sales_price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# similar analysis of price feature as well\n",
    "print(data['sales_price'].describe())\n",
    "# AS we can see that minimum price of product is 39 and max 9988 rupee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.652979Z",
     "iopub.status.busy": "2023-12-09T04:16:16.651882Z",
     "iopub.status.idle": "2023-12-09T04:16:16.678474Z",
     "shell.execute_reply": "2023-12-09T04:16:16.677555Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.652944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count                           19233\n",
      "unique                          15949\n",
      "top       BIBA Women's Straight Kurta\n",
      "freq                               70\n",
      "Name: product_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#similarly for the \"Product Name\"\n",
    "print(data['product_name'].describe())\n",
    "# most common title have womens kurta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Removing Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.679698Z",
     "iopub.status.busy": "2023-12-09T04:16:16.679364Z",
     "iopub.status.idle": "2023-12-09T04:16:16.693870Z",
     "shell.execute_reply": "2023-12-09T04:16:16.692519Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.679664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3284\n"
     ]
    }
   ],
   "source": [
    "# find number of product have the same product name\n",
    "print( sum(data.duplicated('product_name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.700954Z",
     "iopub.status.busy": "2023-12-09T04:16:16.700561Z",
     "iopub.status.idle": "2023-12-09T04:16:16.716869Z",
     "shell.execute_reply": "2023-12-09T04:16:16.715720Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.700920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # Remove the product having Duplicate title\n",
    "data = data.drop_duplicates(subset=['product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.719172Z",
     "iopub.status.busy": "2023-12-09T04:16:16.718400Z",
     "iopub.status.idle": "2023-12-09T04:16:16.750354Z",
     "shell.execute_reply": "2023-12-09T04:16:16.749340Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.719128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14933, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the product with very few words in product name\n",
    "d_data=data[data['product_name'].apply(lambda x: len(x.split())>4)]\n",
    "d_data.shape\n",
    "# around 3284 Data point is removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.751897Z",
     "iopub.status.busy": "2023-12-09T04:16:16.751517Z",
     "iopub.status.idle": "2023-12-09T04:16:16.780583Z",
     "shell.execute_reply": "2023-12-09T04:16:16.779794Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.751869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/3414184361.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d_data.sort_values('product_name',inplace=True,ascending=False)\n"
     ]
    }
   ],
   "source": [
    "#sort the data with their alphabetical order\n",
    "d_data.sort_values('product_name',inplace=True,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.782399Z",
     "iopub.status.busy": "2023-12-09T04:16:16.782074Z",
     "iopub.status.idle": "2023-12-09T04:16:16.789883Z",
     "shell.execute_reply": "2023-12-09T04:16:16.789074Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.782370Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22150                     zs cap house Owise Grey net topi\n",
       "23619                           zs cap house Blue net topi\n",
       "21756    zero by Hopscotch Girls' Cotton Art Print Casu...\n",
       "26085    yazhi Men's Cotton T-Shirt - Be You - Dark Blu...\n",
       "5523       women's manzella cable knit glove (pack of two)\n",
       "Name: product_name, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_data['product_name'].head()\n",
    "# there some differnt type of duplicate like only they are differ by only few words\n",
    "# also they are different only by color or by keyword only\n",
    "# we can remove them as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:16.791713Z",
     "iopub.status.busy": "2023-12-09T04:16:16.791224Z",
     "iopub.status.idle": "2023-12-09T04:16:27.154599Z",
     "shell.execute_reply": "2023-12-09T04:16:27.153716Z",
     "shell.execute_reply.started": "2023-12-09T04:16:16.791683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PreProcssing the text \n",
    "# using nltk library to do it\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk.stem\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:27.156350Z",
     "iopub.status.busy": "2023-12-09T04:16:27.155853Z",
     "iopub.status.idle": "2023-12-09T04:16:30.249487Z",
     "shell.execute_reply": "2023-12-09T04:16:30.247743Z",
     "shell.execute_reply.started": "2023-12-09T04:16:27.156320Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15949/15949 [00:03<00:00, 5198.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation \n",
    "def remove_pun(sentence):\n",
    "    cleaned=re.sub(r'[?|!|\\|#|.|\"|)|(|)|/|,|:|\\'|-|$|+|~|;|-|_|@|>|<]',r'',sentence)\n",
    "    return cleaned\n",
    "#3. Remove word which are not english letter\n",
    "def remove_alhpa(sentence):\n",
    "    cleaned=(\"\\S*\\d\\S*\", \"\", sentence)\n",
    "    return cleaned\n",
    "# all english stop words\n",
    "stopwords= set(stopwords.words('english'))\n",
    "# some phases in written short word \n",
    "def decontracted(phrase):\n",
    "    # specific short to long conversion\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general decontracted\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "# stemming the words\n",
    "sno=nltk.stem.SnowballStemmer('english')\n",
    "# tqdm is for printing the status bar\n",
    "\n",
    "product_name=[]\n",
    "for sentence in tqdm(data['product_name'].values):\n",
    "    #converting the pharases\n",
    "    sentance = decontracted(sentence)\n",
    "    #remove alha numeric\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    #convert every word in lower and take only if not in stop word and then snowball\n",
    "    sentence = \" \".join((sno.stem(e.lower())) for e in sentence.split() if e.lower() not in stopwords)\n",
    "    # append the processed title to prodcut_name \n",
    "    product_name.append(remove_pun(sentence).strip())\n",
    "data['product_name']=product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:30.251190Z",
     "iopub.status.busy": "2023-12-09T04:16:30.250870Z",
     "iopub.status.idle": "2023-12-09T04:16:30.261958Z",
     "shell.execute_reply": "2023-12-09T04:16:30.260599Z",
     "shell.execute_reply.started": "2023-12-09T04:16:30.251164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    la facon cotton kalamkari handblock sare blous...\n",
      "2    lovista cotton gota patti tassel tradit print ...\n",
      "5    forest club  gym wear  sport shorts short men ...\n",
      "6    printoctopus graphic print t-shirt men chill t...\n",
      "9                             peppermint synthet dress\n",
      "Name: product_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['product_name'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Based Product Similarity using Sklearn and BOW , TF-IDF , Word2Vec , BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:30.264425Z",
     "iopub.status.busy": "2023-12-09T04:16:30.263586Z",
     "iopub.status.idle": "2023-12-09T04:16:30.664261Z",
     "shell.execute_reply": "2023-12-09T04:16:30.663120Z",
     "shell.execute_reply.started": "2023-12-09T04:16:30.264389Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse._csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (15949, 14330)\n",
      "the number of unique words  14330\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Step 1 Convert the Text/Title using BOW\n",
    "count_vect = CountVectorizer() #in scikit-learn that do as above description\n",
    "count_vect.fit(data['product_name'])\n",
    "final_counts = count_vect.transform(data['product_name'])\n",
    "print(\"the type of count vectorizer \",type(final_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\n",
    "print(\"the number of unique words \", final_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compute the product similarity using BOW as text to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:50:32.588175Z",
     "iopub.status.busy": "2023-12-09T12:50:32.587764Z",
     "iopub.status.idle": "2023-12-09T12:50:32.598341Z",
     "shell.execute_reply": "2023-12-09T12:50:32.597497Z",
     "shell.execute_reply.started": "2023-12-09T12:50:32.588146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for given a product title find 'x' similar Product \n",
    "# Algorithm\n",
    "# find the pairwise distance bewteen given point and all other point using eculidean distance\n",
    "# sort them using np.argsort\n",
    "#pick the top 'x' title having the smallest distance \n",
    "# print them as a output the Product that can be Recommmended to user based on product\n",
    "# similarity using BOW as a text to vector technique and eculidean distance as the similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "def RecommenderSystems(p_id,numProduct,vectors):\n",
    "    \"\"\"\n",
    "    p_id is the index from data that  is preprocessed\n",
    "    numProduct of wanted to recommeded\n",
    "    \"\"\"\n",
    "    # Compute the Pairwise Distances\n",
    "    dist=pairwise_distances(vectors,vectors[p_id].reshape(1,-1))\n",
    "    # sort them based the distance computed\n",
    "    indices=np.argsort(dist.flatten())[0:numProduct]\n",
    "    # this will store distances between the product_titles\n",
    "    p_dist=np.sort(dist.flatten())[0:numProduct]\n",
    "    # take product details for these indices\n",
    "    similar_product=list(data.index[indices])\n",
    "    for i in range(len(indices)):\n",
    "        # generate the heatmap as well\n",
    "        display(Image(url=data['medium'].loc[similar_product[i]],width=100, height=100))\n",
    "        print('ASIN :',data['asin'].loc[similar_product[i]])\n",
    "        print('Brand :',data['brand'].loc[similar_product[i]])\n",
    "        print('Product Name :',data['product_name'].loc[similar_product[i]])\n",
    "        print('Distance :',p_dist[i])\n",
    "        print('_'*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск по текстовому запросу пользователя\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    \"\"\"\n",
    "    Предобработка текстового запроса пользователя (та же логика, что и для названий товаров)\n",
    "    \"\"\"\n",
    "    sentance = decontracted(query)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    sentence = \" \".join((sno.stem(e.lower())) for e in sentance.split() if e.lower() not in stopwords)\n",
    "    return remove_pun(sentence).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchByQueryBOW(query, numProduct, vectorizer, vectors):\n",
    "    \"\"\"\n",
    "    Поиск товаров по текстовому запросу используя BOW\n",
    "    query - текстовый запрос пользователя\n",
    "    numProduct - количество товаров для возврата\n",
    "    vectorizer - обученный CountVectorizer\n",
    "    vectors - векторы всех товаров (BOW)\n",
    "    \"\"\"\n",
    "    query_processed = preprocess_query(query)\n",
    "    query_vec = vectorizer.transform([query_processed])\n",
    "    \n",
    "    dist = pairwise_distances(vectors, query_vec)\n",
    "    indices = np.argsort(dist.flatten())[0:numProduct]\n",
    "    p_dist = np.sort(dist.flatten())[0:numProduct]\n",
    "    \n",
    "    similar_product = list(data.index[indices])\n",
    "    print(f\"Запрос: '{query}' -> обработан: '{query_processed}'\")\n",
    "    print(f\"Найдено товаров: {numProduct}\\n\")\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        display(Image(url=data['medium'].loc[similar_product[i]], width=100, height=100))\n",
    "        print('ASIN :', data['asin'].loc[similar_product[i]])\n",
    "        print('Brand :', data['brand'].loc[similar_product[i]])\n",
    "        print('Product Name :', data['product_name'].loc[similar_product[i]])\n",
    "        print('Distance :', p_dist[i])\n",
    "        print('_'*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchByQueryTFIDF(query, numProduct, vectorizer, vectors):\n",
    "    \"\"\"\n",
    "    Поиск товаров по текстовому запросу используя TF-IDF\n",
    "    query - текстовый запрос пользователя\n",
    "    numProduct - количество товаров для возврата\n",
    "    vectorizer - обученный TfidfVectorizer\n",
    "    vectors - векторы всех товаров (TF-IDF)\n",
    "    \"\"\"\n",
    "    query_processed = preprocess_query(query)\n",
    "    query_vec = vectorizer.transform([query_processed])\n",
    "    \n",
    "    dist = pairwise_distances(vectors, query_vec)\n",
    "    indices = np.argsort(dist.flatten())[0:numProduct]\n",
    "    p_dist = np.sort(dist.flatten())[0:numProduct]\n",
    "    \n",
    "    similar_product = list(data.index[indices])\n",
    "    print(f\"Запрос: '{query}' -> обработан: '{query_processed}'\")\n",
    "    print(f\"Найдено товаров: {numProduct}\\n\")\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        display(Image(url=data['medium'].loc[similar_product[i]], width=100, height=100))\n",
    "        print('ASIN :', data['asin'].loc[similar_product[i]])\n",
    "        print('Brand :', data['brand'].loc[similar_product[i]])\n",
    "        print('Product Name :', data['product_name'].loc[similar_product[i]])\n",
    "        print('Distance :', p_dist[i])\n",
    "        print('_'*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchByQueryWord2Vec(query, numProduct, w2v_model, w2v_words, vectors):\n",
    "    \"\"\"\n",
    "    Поиск товаров по текстовому запросу используя Word2Vec\n",
    "    query - текстовый запрос пользователя\n",
    "    numProduct - количество товаров для возврата\n",
    "    w2v_model - обученная модель Word2Vec\n",
    "    w2v_words - список слов в модели\n",
    "    vectors - векторы всех товаров (Word2Vec)\n",
    "    \"\"\"\n",
    "    query_processed = preprocess_query(query)\n",
    "    \n",
    "    query_vec = np.zeros(300)\n",
    "    cnt_words = 0\n",
    "    for word in query_processed.split():\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model[word]\n",
    "            query_vec += vec\n",
    "            cnt_words += 1\n",
    "    \n",
    "    if cnt_words == 0:\n",
    "        print(f\"Запрос: '{query}' -> обработан: '{query_processed}'\")\n",
    "        print(\"Ошибка: ни одно слово из запроса не найдено в модели Word2Vec\")\n",
    "        return\n",
    "    \n",
    "    query_vec /= cnt_words\n",
    "    query_vec = query_vec.reshape(1, -1)\n",
    "    \n",
    "    dist = pairwise_distances(vectors, query_vec)\n",
    "    indices = np.argsort(dist.flatten())[0:numProduct]\n",
    "    p_dist = np.sort(dist.flatten())[0:numProduct]\n",
    "    \n",
    "    similar_product = list(data.index[indices])\n",
    "    print(f\"Запрос: '{query}' -> обработан: '{query_processed}'\")\n",
    "    print(f\"Найдено товаров: {numProduct}\\n\")\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        display(Image(url=data['medium'].loc[similar_product[i]], width=100, height=100))\n",
    "        print('ASIN :', data['asin'].loc[similar_product[i]])\n",
    "        print('Brand :', data['brand'].loc[similar_product[i]])\n",
    "        print('Product Name :', data['product_name'].loc[similar_product[i]])\n",
    "        print('Distance :', p_dist[i])\n",
    "        print('_'*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchByQueryTFIDFWord2Vec(query, numProduct, w2v_model, w2v_words, tfidf_dict, tfidf_feat, vectors):\n",
    "    \"\"\"\n",
    "    Поиск товаров по текстовому запросу используя TF-IDF weighted Word2Vec\n",
    "    query - текстовый запрос пользователя\n",
    "    numProduct - количество товаров для возврата\n",
    "    w2v_model - обученная модель Word2Vec\n",
    "    w2v_words - список слов в модели\n",
    "    tfidf_dict - словарь IDF значений\n",
    "    tfidf_feat - список признаков TF-IDF\n",
    "    vectors - векторы всех товаров (TF-IDF weighted Word2Vec)\n",
    "    \"\"\"\n",
    "    query_processed = preprocess_query(query)\n",
    "    \n",
    "    query_vec = np.zeros(300)\n",
    "    weight_sum = 0\n",
    "    query_words = query_processed.split()\n",
    "    \n",
    "    for word in query_words:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model[word]\n",
    "            tf_idf = tfidf_dict[word] * (query_words.count(word) / len(query_words))\n",
    "            query_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    \n",
    "    if weight_sum == 0:\n",
    "        print(f\"Запрос: '{query}' -> обработан: '{query_processed}'\")\n",
    "        print(\"Ошибка: не удалось создать вектор для запроса\")\n",
    "        return\n",
    "    \n",
    "    query_vec /= weight_sum\n",
    "    query_vec = query_vec.reshape(1, -1)\n",
    "    \n",
    "    dist = pairwise_distances(vectors, query_vec)\n",
    "    indices = np.argsort(dist.flatten())[0:numProduct]\n",
    "    p_dist = np.sort(dist.flatten())[0:numProduct]\n",
    "    \n",
    "    similar_product = list(data.index[indices])\n",
    "    print(f\"Запрос: '{query}' -> обработан: '{query_processed}'\")\n",
    "    print(f\"Найдено товаров: {numProduct}\\n\")\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        display(Image(url=data['medium'].loc[similar_product[i]], width=100, height=100))\n",
    "        print('ASIN :', data['asin'].loc[similar_product[i]])\n",
    "        print('Brand :', data['brand'].loc[similar_product[i]])\n",
    "        print('Product Name :', data['product_name'].loc[similar_product[i]])\n",
    "        print('Distance :', p_dist[i])\n",
    "        print('_'*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchByQueryWeighted(query, numProduct, w2v_model, w2v_words, tfidf_dict, tfidf_feat, \n",
    "                          text_vectors, brand_vectors, category_weights):\n",
    "    \"\"\"\n",
    "    Поиск товаров по текстовому запросу с использованием взвешенных признаков\n",
    "    query - текстовый запрос пользователя\n",
    "    numProduct - количество товаров для возврата\n",
    "    text_vectors - векторы текстов (TF-IDF weighted Word2Vec)\n",
    "    brand_vectors - векторы брендов (One-Hot)\n",
    "    category_weights - веса для каждого признака [текст, бренд]\n",
    "    \"\"\"\n",
    "    query_processed = preprocess_query(query)\n",
    "    \n",
    "    query_text_vec = np.zeros(300)\n",
    "    weight_sum = 0\n",
    "    query_words = query_processed.split()\n",
    "    \n",
    "    for word in query_words:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model[word]\n",
    "            tf_idf = tfidf_dict[word] * (query_words.count(word) / len(query_words))\n",
    "            query_text_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    \n",
    "    if weight_sum == 0:\n",
    "        print(f\"Запрос: '{query}' -> обработан: '{query_processed}'\")\n",
    "        print(\"Ошибка: не удалось создать вектор для запроса\")\n",
    "        return\n",
    "    \n",
    "    query_text_vec /= weight_sum\n",
    "    query_text_vec = query_text_vec * category_weights[0]\n",
    "    \n",
    "    query_brand_vec = np.zeros(brand_vectors.shape[1]) * category_weights[1]\n",
    "    \n",
    "    query_vec = np.concatenate([query_text_vec, query_brand_vec]).reshape(1, -1)\n",
    "    \n",
    "    weighted_vectors = []\n",
    "    for i in range(len([text_vectors, brand_vectors])):\n",
    "        vec = np.array([text_vectors, brand_vectors][i]) * category_weights[i]\n",
    "        weighted_vectors.append(vec)\n",
    "    all_vectors = np.concatenate(weighted_vectors, axis=1)\n",
    "    \n",
    "    dist = pairwise_distances(all_vectors, query_vec)\n",
    "    indices = np.argsort(dist.flatten())[0:numProduct]\n",
    "    p_dist = np.sort(dist.flatten())[0:numProduct]\n",
    "    \n",
    "    similar_product = list(data.index[indices])\n",
    "    print(f\"Запрос: '{query}' -> обработан: '{query_processed}'\")\n",
    "    print(f\"Найдено товаров: {numProduct}\\n\")\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        display(Image(url=data['medium'].loc[similar_product[i]], width=100, height=100))\n",
    "        print('ASIN :', data['asin'].loc[similar_product[i]])\n",
    "        print('Brand :', data['brand'].loc[similar_product[i]])\n",
    "        print('Product Name :', data['product_name'].loc[similar_product[i]])\n",
    "        print('Distance :', p_dist[i])\n",
    "        print('_'*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры использования поиска по запросу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример 1: Поиск по BOW\n",
    "SearchByQueryBOW(\"women cotton dress\", 5, count_vect, final_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример 2: Поиск по TF-IDF\n",
    "SearchByQueryTFIDF(\"men t-shirt\", 5, tifd, tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример 3: Поиск по Word2Vec\n",
    "SearchByQueryWord2Vec(\"baseball cap\", 5, w2v_model, w2v_words, pv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример 4: Поиск по TF-IDF weighted Word2Vec (рекомендуется)\n",
    "SearchByQueryTFIDFWord2Vec(\"kurta women\", 5, w2v_model, w2v_words, dictionary, tfidf_feat, tfidf_sent_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример 5: Поиск с учетом текста и бренда (взвешенный)\n",
    "SearchByQueryWeighted(\"handloom saree\", 5, w2v_model, w2v_words, dictionary, tfidf_feat, \n",
    "                      tfidf_sent_vectors, brand_vec.values, [1, 0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачивание необходимых ресурсов NLTK\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"Stopwords уже загружены\")\n",
    "except LookupError:\n",
    "    print(\"Скачиваю stopwords...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"Stopwords загружены!\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Punkt tokenizer уже загружен\")\n",
    "except LookupError:\n",
    "    print(\"Скачиваю punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"Punkt tokenizer загружен!\")\n",
    "\n",
    "print(\"Все необходимые ресурсы NLTK готовы!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка наличия необходимых переменных и их создание при необходимости\n",
    "try:\n",
    "    # Проверяем наличие tifd и tf\n",
    "    if 'tifd' not in globals() or 'tf' not in globals():\n",
    "        print(\"Создаю TF-IDF векторизатор...\")\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tifd = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "        tf = tifd.fit_transform(data['product_name'])\n",
    "        print(f\"TF-IDF векторизатор создан. Размерность: {tf.get_shape()}\")\n",
    "    else:\n",
    "        print(\"TF-IDF векторизатор уже существует\")\n",
    "    \n",
    "    # Проверяем наличие функции preprocess_query и создаем простую версию, если её нет\n",
    "    if 'preprocess_query' not in globals():\n",
    "        print(\"Создаю упрощенную версию preprocess_query...\")\n",
    "        import re\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        import nltk.stem\n",
    "        \n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "        \n",
    "        stopwords_set = set(stopwords.words('english'))\n",
    "        sno = nltk.stem.SnowballStemmer('english')\n",
    "        \n",
    "        def decontracted(phrase):\n",
    "            phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "            phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "            phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "            phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "            phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "            phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "            phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "            phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "            return phrase\n",
    "        \n",
    "        def remove_pun(sentence):\n",
    "            cleaned = re.sub(r'[?|!|\\|#|.|\"|)|(|)|/|,|:|\\'|-|$|+|~|;|-|_|@|>|<]', r'', sentence)\n",
    "            return cleaned\n",
    "        \n",
    "        def preprocess_query(query):\n",
    "            sentance = decontracted(query)\n",
    "            sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "            sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "            sentence = \" \".join((sno.stem(e.lower())) for e in sentance.split() if e.lower() not in stopwords_set)\n",
    "            return remove_pun(sentence).strip()\n",
    "        \n",
    "        print(\"Функция preprocess_query создана\")\n",
    "    else:\n",
    "        print(\"Функция preprocess_query найдена\")\n",
    "        \n",
    "except NameError as e:\n",
    "    print(f\"Ошибка: {e}\")\n",
    "    print(\"Убедитесь, что переменная 'data' определена (загружены данные)\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def generate_feed_tfidf(query, n_products, vectorizer, product_vectors, data_df, rows=2):\n",
    "    \"\"\"\n",
    "    Генерирует выдачу товаров на основе TF-IDF эмбеддингов\n",
    "    \n",
    "    Параметры:\n",
    "    -----------\n",
    "    query : str\n",
    "        Текстовый запрос пользователя\n",
    "    n_products : int\n",
    "        Количество товаров для выдачи (должно быть кратно rows*2)\n",
    "    vectorizer : TfidfVectorizer\n",
    "        Обученный TF-IDF векторизатор\n",
    "    product_vectors : scipy.sparse matrix\n",
    "        TF-IDF векторы всех товаров\n",
    "    data_df : pandas.DataFrame\n",
    "        DataFrame с данными о товарах\n",
    "    rows : int\n",
    "        Количество строк в выдаче (по умолчанию 2, итого rows*2 товаров)\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    results : list of dict\n",
    "        Список словарей с информацией о товарах и коэффициентом релевантности\n",
    "    \"\"\"\n",
    "    # Предобработка запроса\n",
    "    query_processed = preprocess_query(query)\n",
    "    \n",
    "    # Векторизация запроса\n",
    "    query_vec = vectorizer.transform([query_processed])\n",
    "    \n",
    "    # Вычисление косинусного сходства между запросом и всеми товарами\n",
    "    similarities = cosine_similarity(product_vectors, query_vec).flatten()\n",
    "    \n",
    "    # Нормализация в диапазон [0, 1]\n",
    "    # Косинусное сходство уже в диапазоне [-1, 1], нормализуем в [0, 1]\n",
    "    min_sim = similarities.min()\n",
    "    max_sim = similarities.max()\n",
    "    if max_sim - min_sim > 0:\n",
    "        relevance_scores = (similarities - min_sim) / (max_sim - min_sim)\n",
    "    else:\n",
    "        relevance_scores = np.ones_like(similarities)\n",
    "    \n",
    "    # Выбор топ-n товаров\n",
    "    top_indices = np.argsort(relevance_scores)[::-1][:n_products]\n",
    "    \n",
    "    # Формирование результатов (только id, название, изображение)\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        product_idx = data_df.index[idx]\n",
    "        results.append({\n",
    "            'id': data_df.loc[product_idx, 'asin'],\n",
    "            'product_name': data_df.loc[product_idx, 'product_name'],\n",
    "            'image_url': data_df.loc[product_idx, 'medium'],\n",
    "            'relevance_score': float(relevance_scores[idx]),\n",
    "            'vector_index': int(idx)  # для вычисления переходов\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_feed_grid(results, rows=2, cols=2):\n",
    "    \"\"\"\n",
    "    Отображает выдачу товаров в виде сетки l*2\n",
    "    \n",
    "    Параметры:\n",
    "    -----------\n",
    "    results : list of dict\n",
    "        Список результатов от generate_feed_tfidf\n",
    "    rows : int\n",
    "        Количество строк\n",
    "    cols : int\n",
    "        Количество колонок (обычно 2)\n",
    "    \"\"\"\n",
    "    from IPython.display import display, HTML, Image\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(f\"Выдача товаров ({rows} строки x {cols} колонки):\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(0, len(results), cols):\n",
    "        row_products = results[i:i+cols]\n",
    "        print(f\"\\nСтрока {i//cols + 1}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for j, product in enumerate(row_products):\n",
    "            print(f\"\\nТовар {i+j+1} (колонка {j+1}):\")\n",
    "            print(f\"  ID: {product['id']}\")\n",
    "            print(f\"  Название: {product['product_name']}\")\n",
    "            if 'purchase_probability' in product:\n",
    "                print(f\"  Вероятность покупки: {product['purchase_probability']:.4f}\")\n",
    "            if product['image_url']:\n",
    "                try:\n",
    "                    display(Image(url=product['image_url'], width=150, height=150))\n",
    "                except:\n",
    "                    print(f\"  Изображение: {product['image_url']}\")\n",
    "            print()\n",
    "        \n",
    "        if i + cols < len(results):\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Сводная таблица\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Сводная таблица результатов:\")\n",
    "    print(\"=\" * 80)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    columns_to_show = ['id', 'product_name', 'image_url']\n",
    "    if 'purchase_probability' in df_results.columns:\n",
    "        columns_to_show.append('purchase_probability')\n",
    "    display(df_results[columns_to_show])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования модели\n",
    "query = \"women cotton dress\"\n",
    "n_products = 4  # 2 строки * 2 колонки\n",
    "rows = 2\n",
    "\n",
    "results = generate_feed_tfidf(query, n_products, tifd, tf, data, rows=rows)\n",
    "display_feed_grid(results, rows=rows, cols=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_probabilities(results, data_df, rows=2, cols=2, image_model=None):\n",
    "    \"\"\"\n",
    "    Вычисляет вероятности переходов между соседними товарами в сетке на основе схожести эмбеддингов изображений\n",
    "    \n",
    "    Параметры:\n",
    "    -----------\n",
    "    results : list of dict\n",
    "        Список товаров из generate_feed_tfidf (содержит id, product_name, image_url)\n",
    "    data_df : pandas.DataFrame\n",
    "        DataFrame с данными о товарах\n",
    "    rows : int\n",
    "        Количество строк в сетке\n",
    "    cols : int\n",
    "        Количество колонок в сетке\n",
    "    image_model : tensorflow model, optional\n",
    "        Модель VGG16 для извлечения эмбеддингов изображений. Если None, попытается загрузить.\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    transitions : dict\n",
    "        Словарь с вероятностями переходов:\n",
    "        - 'down': вероятности переходов вниз (для каждого товара)\n",
    "        - 'left': вероятности переходов налево\n",
    "        - 'right': вероятности переходов направо\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.applications import VGG16\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.utils import get_file\n",
    "    \n",
    "    # Загружаем модель VGG16 если не передана\n",
    "    if image_model is None:\n",
    "        try:\n",
    "            image_model = VGG16(weights='imagenet', include_top=False)\n",
    "            print(\"Модель VGG16 загружена\")\n",
    "        except:\n",
    "            print(\"Ошибка загрузки VGG16. Используем эмбеддинги названий как fallback.\")\n",
    "            image_model = None\n",
    "    \n",
    "    def extract_image_embedding(img_url, item_id):\n",
    "        \"\"\"Извлекает эмбеддинг изображения\"\"\"\n",
    "        if image_model is None:\n",
    "            return None\n",
    "        try:\n",
    "            # Обрабатываем URL - может быть несколько через |\n",
    "            if '|' in str(img_url):\n",
    "                img_url = str(img_url).split('|')[0]\n",
    "            \n",
    "            # Используем уникальное имя файла для каждого товара\n",
    "            import hashlib\n",
    "            url_hash = hashlib.md5(str(img_url).encode()).hexdigest()[:8]\n",
    "            temp_filename = f'temp_image_{item_id}_{url_hash}.jpg'\n",
    "            \n",
    "            img_path = get_file(temp_filename, origin=img_url)\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            features = image_model.predict(img_array, verbose=0)\n",
    "            embedding = features.flatten()\n",
    "            \n",
    "            # Проверяем, что эмбеддинг не нулевой\n",
    "            if np.allclose(embedding, 0):\n",
    "                print(f\"    ВНИМАНИЕ: Эмбеддинг нулевой для {item_id}\")\n",
    "                return None\n",
    "            \n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"    Ошибка обработки изображения {item_id} ({img_url[:50]}...): {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Извлекаем эмбеддинги изображений для всех товаров в выдаче\n",
    "    print(\"Извлечение эмбеддингов изображений...\")\n",
    "    print(f\"URL изображений:\")\n",
    "    for i, item in enumerate(results):\n",
    "        print(f\"  Товар {i+1} ({item['id']}): {str(item['image_url'])[:100]}...\")\n",
    "    \n",
    "    image_embeddings = []\n",
    "    for i, item in enumerate(results):\n",
    "        embedding = extract_image_embedding(item['image_url'], item['id'])\n",
    "        if embedding is not None:\n",
    "            image_embeddings.append(embedding)\n",
    "            print(f\"  Товар {i+1} ({item['id']}): эмбеддинг извлечен, размер={len(embedding)}, norm={np.linalg.norm(embedding):.4f}, min={embedding.min():.4f}, max={embedding.max():.4f}\")\n",
    "        else:\n",
    "            # Fallback: используем случайный вектор, чтобы они были разные\n",
    "            import random\n",
    "            random.seed(hash(item['id']) % 1000)\n",
    "            fallback_emb = np.random.rand(25088) * 0.01  # маленькие случайные значения\n",
    "            image_embeddings.append(fallback_emb)\n",
    "            print(f\"  Товар {i+1} ({item['id']}): ошибка извлечения, используется случайный вектор\")\n",
    "    \n",
    "    image_embeddings = np.array(image_embeddings)\n",
    "    \n",
    "    # Проверяем, что эмбеддинги разные\n",
    "    print(f\"\\nПроверка эмбеддингов:\")\n",
    "    print(f\"  Размер массива: {image_embeddings.shape}\")\n",
    "    norms = [np.linalg.norm(emb) for emb in image_embeddings]\n",
    "    print(f\"  Норма каждого эмбеддинга: {norms}\")\n",
    "    \n",
    "    # Проверяем, не все ли эмбеддинги одинаковые\n",
    "    if len(image_embeddings) > 1:\n",
    "        first_emb = image_embeddings[0]\n",
    "        all_same = True\n",
    "        differences = []\n",
    "        for i, emb in enumerate(image_embeddings[1:], 1):\n",
    "            diff = np.linalg.norm(emb - first_emb)\n",
    "            differences.append(diff)\n",
    "            if not np.allclose(emb, first_emb, atol=1e-6):\n",
    "                all_same = False\n",
    "        \n",
    "        print(f\"  Все эмбеддинги одинаковые: {all_same}\")\n",
    "        print(f\"  Различия между эмбеддингами: {differences}\")\n",
    "        if all_same:\n",
    "            print(\"  ВНИМАНИЕ: Все эмбеддинги одинаковые! Возможно, изображения не загружаются или кэшируются.\")\n",
    "        else:\n",
    "            print(f\"  Эмбеддинги разные. Минимальная разница: {min(differences):.6f}, максимальная: {max(differences):.6f}\")\n",
    "    \n",
    "    # Вычисляем матрицу схожести между эмбеддингами изображений\n",
    "    similarity_matrix = cosine_similarity(image_embeddings)\n",
    "    print(f\"\\nМатрица схожести вычислена. Размер: {similarity_matrix.shape}\")\n",
    "    print(f\"Диапазон схожести: min={similarity_matrix.min():.6f}, max={similarity_matrix.max():.6f}\")\n",
    "    print(f\"Диагональ (схожесть товара с самим собой): {np.diag(similarity_matrix)}\")\n",
    "    print(f\"Полная матрица схожести:\\n{similarity_matrix}\")\n",
    "    \n",
    "    # Косинусное сходство уже в диапазоне [-1, 1], но для изображений обычно [0, 1]\n",
    "    # Нормализуем в [0, 1] если нужно\n",
    "    min_sim_global = similarity_matrix.min()\n",
    "    max_sim_global = similarity_matrix.max()\n",
    "    \n",
    "    print(f\"\\nНормализация:\")\n",
    "    print(f\"  min_sim_global={min_sim_global:.6f}, max_sim_global={max_sim_global:.6f}\")\n",
    "    \n",
    "    # Если все значения одинаковые (например, все 1.0), это проблема\n",
    "    if abs(max_sim_global - min_sim_global) < 1e-6:\n",
    "        print(\"  ВНИМАНИЕ: Все схожести одинаковые! Используем схожесть напрямую без нормализации.\")\n",
    "        similarity_matrix_normalized = similarity_matrix\n",
    "    elif min_sim_global < 0:\n",
    "        similarity_matrix_normalized = (similarity_matrix - min_sim_global) / (max_sim_global - min_sim_global)\n",
    "        print(f\"  Нормализация с отрицательными значениями применена\")\n",
    "    else:\n",
    "        # Если все значения положительные, нормализуем относительно максимума\n",
    "        if max_sim_global > 0:\n",
    "            similarity_matrix_normalized = similarity_matrix / max_sim_global\n",
    "            print(f\"  Нормализация делением на максимум: {max_sim_global:.6f}\")\n",
    "        else:\n",
    "            similarity_matrix_normalized = similarity_matrix\n",
    "            print(f\"  Нормализация не применена (max=0)\")\n",
    "    \n",
    "    print(f\"После нормализации: min={similarity_matrix_normalized.min():.6f}, max={similarity_matrix_normalized.max():.6f}\")\n",
    "    print(f\"Нормализованная матрица:\\n{similarity_matrix_normalized}\")\n",
    "    \n",
    "    transitions = {\n",
    "        'down': {},  # переход вниз: (row, col) -> (row+1, col)\n",
    "        'left': {},  # переход налево: (row, col) -> (row, col-1)\n",
    "        'right': {}  # переход направо: (row, col) -> (row, col+1)\n",
    "    }\n",
    "    \n",
    "    # Проходим по сетке и вычисляем вероятности переходов\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            idx = row * cols + col\n",
    "            if idx >= len(results):\n",
    "                continue\n",
    "            \n",
    "            current_similarities = similarity_matrix[idx]\n",
    "            \n",
    "            # Переход вниз\n",
    "            if row < rows - 1:\n",
    "                down_idx = (row + 1) * cols + col\n",
    "                if down_idx < len(results):\n",
    "                    # Вероятность = нормализованная схожесть эмбеддингов\n",
    "                    similarity = similarity_matrix[idx, down_idx]\n",
    "                    prob = float(similarity_matrix_normalized[idx, down_idx])\n",
    "                    transitions['down'][(row, col)] = {\n",
    "                        'to': (row + 1, col),\n",
    "                        'probability': prob,\n",
    "                        'similarity': float(similarity),\n",
    "                        'from_id': results[idx]['id'],\n",
    "                        'to_id': results[down_idx]['id']\n",
    "                    }\n",
    "            \n",
    "            # Переход налево\n",
    "            if col > 0:\n",
    "                left_idx = row * cols + (col - 1)\n",
    "                if left_idx < len(results):\n",
    "                    similarity = similarity_matrix[idx, left_idx]\n",
    "                    prob = float(similarity_matrix_normalized[idx, left_idx])\n",
    "                    transitions['left'][(row, col)] = {\n",
    "                        'to': (row, col - 1),\n",
    "                        'probability': prob,\n",
    "                        'similarity': float(similarity),\n",
    "                        'from_id': results[idx]['id'],\n",
    "                        'to_id': results[left_idx]['id']\n",
    "                    }\n",
    "            \n",
    "            # Переход направо\n",
    "            if col < cols - 1:\n",
    "                right_idx = row * cols + (col + 1)\n",
    "                if right_idx < len(results):\n",
    "                    similarity = similarity_matrix[idx, right_idx]\n",
    "                    prob = float(similarity_matrix_normalized[idx, right_idx])\n",
    "                    transitions['right'][(row, col)] = {\n",
    "                        'to': (row, col + 1),\n",
    "                        'probability': prob,\n",
    "                        'similarity': float(similarity),\n",
    "                        'from_id': results[idx]['id'],\n",
    "                        'to_id': results[right_idx]['id']\n",
    "                    }\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "def display_transition_probabilities(transitions, results, rows=2, cols=2):\n",
    "    \"\"\"\n",
    "    Отображает вероятности переходов в удобном виде\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ВЕРОЯТНОСТИ ПЕРЕХОДОВ МЕЖДУ ТОВАРАМИ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Создаем сетку для визуализации\n",
    "    grid = [[None for _ in range(cols)] for _ in range(rows)]\n",
    "    for i, item in enumerate(results):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        if row < rows and col < cols:\n",
    "            grid[row][col] = item\n",
    "    \n",
    "    print(\"\\nСетка товаров:\")\n",
    "    print(\"-\" * 80)\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            if grid[row][col]:\n",
    "                print(f\"({row}, {col}): {grid[row][col]['id']} - {grid[row][col]['product_name'][:50]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ПЕРЕХОДЫ ВНИЗ:\")\n",
    "    print(\"=\" * 80)\n",
    "    for (from_row, from_col), info in transitions['down'].items():\n",
    "        print(f\"Из ({from_row}, {from_col}) -> ({info['to'][0]}, {info['to'][1]}):\")\n",
    "        print(f\"  Вероятность: {info['probability']:.4f}\")\n",
    "        print(f\"  Схожесть эмбеддингов: {info['similarity']:.4f}\")\n",
    "        print(f\"  От: {info['from_id']}\")\n",
    "        print(f\"  К: {info['to_id']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ПЕРЕХОДЫ НАЛЕВО:\")\n",
    "    print(\"=\" * 80)\n",
    "    for (from_row, from_col), info in transitions['left'].items():\n",
    "        print(f\"Из ({from_row}, {from_col}) -> ({info['to'][0]}, {info['to'][1]}):\")\n",
    "        print(f\"  Вероятность: {info['probability']:.4f}\")\n",
    "        print(f\"  Схожесть эмбеддингов: {info['similarity']:.4f}\")\n",
    "        print(f\"  От: {info['from_id']}\")\n",
    "        print(f\"  К: {info['to_id']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ПЕРЕХОДЫ НАПРАВО:\")\n",
    "    print(\"=\" * 80)\n",
    "    for (from_row, from_col), info in transitions['right'].items():\n",
    "        print(f\"Из ({from_row}, {from_col}) -> ({info['to'][0]}, {info['to'][1]}):\")\n",
    "        print(f\"  Вероятность: {info['probability']:.4f}\")\n",
    "        print(f\"  Схожесть эмбеддингов: {info['similarity']:.4f}\")\n",
    "        print(f\"  От: {info['from_id']}\")\n",
    "        print(f\"  К: {info['to_id']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример: вычисление вероятностей переходов\n",
    "query = \"women cotton dress\"\n",
    "n_products = 4\n",
    "rows = 2\n",
    "cols = 2\n",
    "\n",
    "# Генерируем выдачу\n",
    "results = generate_feed_tfidf(query, n_products, tifd, tf, data, rows=rows)\n",
    "\n",
    "# Вычисляем вероятности переходов на основе эмбеддингов изображений\n",
    "transitions = calculate_transition_probabilities(results, data, rows=rows, cols=cols)\n",
    "\n",
    "# Отображаем результаты\n",
    "display_transition_probabilities(transitions, results, rows=rows, cols=cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление вероятности покупки товара на основе схожести запроса и названия\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка sentence-transformers (если не установлен)\n",
    "try:\n",
    "    import sentence_transformers\n",
    "except ImportError:\n",
    "    print(\"Установка sentence-transformers...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\", \"-q\"])\n",
    "    print(\"sentence-transformers установлен!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переопределяем функции calculate_purchase_probability и get_embedding_model\n",
    "# Глобальная переменная для кэширования модели эмбеддингов\n",
    "_cached_embedding_model = None\n",
    "\n",
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Получает готовую модель для эмбеддингов предложений\n",
    "    Использует sentence-transformers (лучший вариант) или Universal Sentence Encoder\n",
    "    \"\"\"\n",
    "    global _cached_embedding_model\n",
    "    \n",
    "    if _cached_embedding_model is not None:\n",
    "        return _cached_embedding_model\n",
    "    \n",
    "    # Пробуем sentence-transformers (лучший вариант)\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        print(\"Загрузка модели sentence-transformers...\")\n",
    "        _cached_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"Модель sentence-transformers загружена (all-MiniLM-L6-v2)\")\n",
    "        return _cached_embedding_model\n",
    "    except ImportError:\n",
    "        print(\"sentence-transformers не установлен. Пропускаем...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки sentence-transformers: {e}\")\n",
    "        print(\"Пробуем другие модели...\")\n",
    "    \n",
    "    # Fallback: пробуем Universal Sentence Encoder через tensorflow_hub\n",
    "    try:\n",
    "        import tensorflow_hub as hub\n",
    "        print(\"Загрузка Universal Sentence Encoder...\")\n",
    "        _cached_embedding_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        print(\"Universal Sentence Encoder загружен\")\n",
    "        return _cached_embedding_model\n",
    "    except ImportError:\n",
    "        print(\"tensorflow_hub не установлен. Установите: pip install tensorflow-hub\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки Universal Sentence Encoder: {e}\")\n",
    "    \n",
    "    # Последний fallback: пробуем загрузить предобученный Word2Vec\n",
    "    try:\n",
    "        from gensim.models import KeyedVectors\n",
    "        print(\"Попытка загрузки предобученной модели Word2Vec...\")\n",
    "        # Проверяем, существует ли файл\n",
    "        import os\n",
    "        w2v_path = '/kaggle/input/gensimmodel/GoogleNews-vectors-negative300.bin'\n",
    "        if os.path.exists(w2v_path):\n",
    "            _cached_embedding_model = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "            print(\"Предобученная модель Word2Vec загружена\")\n",
    "            return _cached_embedding_model\n",
    "        else:\n",
    "            # Пробуем использовать уже загруженную модель, если она есть в глобальной области\n",
    "            if 'w2v_model' in globals():\n",
    "                print(\"Используем уже загруженную модель Word2Vec из глобальной области\")\n",
    "                _cached_embedding_model = globals()['w2v_model']\n",
    "                return _cached_embedding_model\n",
    "            else:\n",
    "                print(f\"Файл Word2Vec не найден: {w2v_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки Word2Vec: {e}\")\n",
    "        # Пробуем использовать уже загруженную модель, если она есть\n",
    "        if 'w2v_model' in globals():\n",
    "            print(\"Используем уже загруженную модель Word2Vec из глобальной области\")\n",
    "            _cached_embedding_model = globals()['w2v_model']\n",
    "            return _cached_embedding_model\n",
    "    \n",
    "    print(\"Не удалось загрузить ни одну модель эмбеддингов. Будет использован TF-IDF fallback.\")\n",
    "    return None\n",
    "\n",
    "def calculate_purchase_probability(query, product_name, embedding_model=None):\n",
    "    \"\"\"\n",
    "    Вычисляет вероятность покупки товара на основе схожести текста запроса и названия товара\n",
    "    Использует готовую модель для эмбеддингов (sentence-transformers, Universal Sentence Encoder или Word2Vec)\n",
    "    \n",
    "    Параметры:\n",
    "    -----------\n",
    "    query : str\n",
    "        Текстовый запрос пользователя\n",
    "    product_name : str\n",
    "        Название товара\n",
    "    embedding_model : optional\n",
    "        Модель для эмбеддингов. Если None, попытается загрузить готовую модель.\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    probability : float\n",
    "        Вероятность покупки в диапазоне [0, 1]\n",
    "    similarity : float\n",
    "        Схожесть эмбеддингов (до нормализации)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    \n",
    "    # Получаем модель эмбеддингов\n",
    "    if embedding_model is None:\n",
    "        embedding_model = get_embedding_model()\n",
    "        if embedding_model is None:\n",
    "            # Fallback: используем простой TF-IDF\n",
    "            try:\n",
    "                return calculate_purchase_probability_tfidf(query, product_name, None, None, data if 'data' in globals() else None)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка в TF-IDF fallback: {e}\")\n",
    "                return 0.5, 0.0  # Возвращаем среднее значение\n",
    "    \n",
    "    # Определяем тип модели и получаем эмбеддинги\n",
    "    try:\n",
    "        # sentence-transformers\n",
    "        if hasattr(embedding_model, 'encode'):\n",
    "            query_embedding = embedding_model.encode([query])[0]\n",
    "            product_embedding = embedding_model.encode([product_name])[0]\n",
    "        \n",
    "        # Universal Sentence Encoder (tensorflow_hub)\n",
    "        elif callable(embedding_model) and not hasattr(embedding_model, 'vector_size'):\n",
    "            query_embedding = embedding_model([query]).numpy()[0]\n",
    "            product_embedding = embedding_model([product_name]).numpy()[0]\n",
    "        \n",
    "        # Word2Vec (gensim) - используем средний эмбеддинг слов\n",
    "        elif hasattr(embedding_model, 'vector_size') or hasattr(embedding_model, 'index_to_key'):\n",
    "            query_processed = preprocess_query(query)\n",
    "            product_processed = preprocess_query(product_name)\n",
    "            \n",
    "            def get_word2vec_embedding(text, model):\n",
    "                words = text.split()\n",
    "                if len(words) == 0:\n",
    "                    return np.zeros(model.vector_size if hasattr(model, 'vector_size') else 300)\n",
    "                \n",
    "                embeddings = []\n",
    "                for word in words:\n",
    "                    try:\n",
    "                        if hasattr(model, 'index_to_key'):\n",
    "                            # KeyedVectors\n",
    "                            if word in model.index_to_key:\n",
    "                                embeddings.append(model[word])\n",
    "                        else:\n",
    "                            # Word2Vec model\n",
    "                            if word in model.wv:\n",
    "                                embeddings.append(model.wv[word])\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if len(embeddings) == 0:\n",
    "                    return np.zeros(model.vector_size if hasattr(model, 'vector_size') else 300)\n",
    "                \n",
    "                return np.mean(embeddings, axis=0)\n",
    "            \n",
    "            query_embedding = get_word2vec_embedding(query_processed, embedding_model)\n",
    "            product_embedding = get_word2vec_embedding(product_processed, embedding_model)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Неизвестный тип модели эмбеддингов\")\n",
    "        \n",
    "        # Вычисляем косинусное сходство\n",
    "        similarity = cosine_similarity([query_embedding], [product_embedding])[0][0]\n",
    "        \n",
    "        # Нормализуем схожесть в вероятность [0, 1]\n",
    "        # Косинусное сходство в диапазоне [-1, 1], нормализуем в [0, 1]\n",
    "        probability = (similarity + 1) / 2\n",
    "        \n",
    "        return float(probability), float(similarity)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка вычисления эмбеддингов: {e}\")\n",
    "        # Fallback на TF-IDF\n",
    "        try:\n",
    "            return calculate_purchase_probability_tfidf(query, product_name, None, None, data if 'data' in globals() else None)\n",
    "        except:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "def calculate_purchase_probability_tfidf(query, product_name, tfidf_dict=None, tfidf_feat=None, data_df=None):\n",
    "    \"\"\"\n",
    "    Fallback функция: вычисляет вероятность покупки используя только TF-IDF\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    \n",
    "    if tfidf_dict is None or tfidf_feat is None:\n",
    "        if data_df is not None:\n",
    "            tifd_temp = TfidfVectorizer()\n",
    "            tifd_temp.fit_transform(data_df['product_name'])\n",
    "            tfidf_dict = dict(zip(tifd_temp.get_feature_names_out(), list(tifd_temp.idf_)))\n",
    "            tfidf_feat = tifd_temp.get_feature_names_out()\n",
    "        else:\n",
    "            return 0.0, 0.0\n",
    "    \n",
    "    query_processed = preprocess_query(query)\n",
    "    product_processed = preprocess_query(product_name)\n",
    "    \n",
    "    # Создаем простые TF-IDF векторы\n",
    "    query_words = query_processed.split()\n",
    "    product_words = product_processed.split()\n",
    "    \n",
    "    # Вычисляем TF-IDF вручную\n",
    "    def get_tfidf_vector(words, tfidf_dict, tfidf_feat):\n",
    "        vec = np.zeros(len(tfidf_feat))\n",
    "        word_to_idx = {word: idx for idx, word in enumerate(tfidf_feat)}\n",
    "        for word in words:\n",
    "            if word in word_to_idx:\n",
    "                tf = words.count(word) / len(words) if len(words) > 0 else 0\n",
    "                idf = tfidf_dict.get(word, 1.0)\n",
    "                vec[word_to_idx[word]] = tf * idf\n",
    "        return vec\n",
    "    \n",
    "    query_vec = get_tfidf_vector(query_words, tfidf_dict, tfidf_feat)\n",
    "    product_vec = get_tfidf_vector(product_words, tfidf_dict, tfidf_feat)\n",
    "    \n",
    "    similarity = cosine_similarity([query_vec], [product_vec])[0][0]\n",
    "    probability = (similarity + 1) / 2\n",
    "    \n",
    "    return float(probability), float(similarity)\n",
    "\n",
    "print(\"Функции calculate_purchase_probability и get_embedding_model определены\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feed_matrix(query, n_products, vectorizer, product_vectors, data_df, rows=2, cols=2, embedding_model=None, image_model=None):\n",
    "    \"\"\"\n",
    "    Генерирует матрицу размера n * 2, где каждая ячейка содержит:\n",
    "    - id: идентификатор товара\n",
    "    - P_click: схожесть товара с запросом от 0 до 1 (TF-IDF)\n",
    "    - P_buy: схожесть товара с запросом от 0 до 1 (sentence-transformers или другая модель)\n",
    "    - P_look1: схожесть с товаром сверху по изображениям от 0 до 1\n",
    "    - P_look2: схожесть с товаром слева/справа по изображениям от 0 до 1\n",
    "    \n",
    "    Параметры:\n",
    "    -----------\n",
    "    query : str\n",
    "        Текстовый запрос пользователя\n",
    "    n_products : int\n",
    "        Количество товаров (должно быть rows * cols)\n",
    "    vectorizer : TfidfVectorizer\n",
    "        Векторизатор TF-IDF\n",
    "    product_vectors : sparse matrix\n",
    "        TF-IDF векторы товаров\n",
    "    data_df : pandas.DataFrame\n",
    "        DataFrame с данными о товарах\n",
    "    rows : int\n",
    "        Количество строк в матрице\n",
    "    cols : int\n",
    "        Количество колонок в матрице (обычно 2)\n",
    "    embedding_model : optional\n",
    "        Модель для эмбеддингов (sentence-transformers) для P_buy\n",
    "    image_model : optional\n",
    "        Модель VGG16 для извлечения эмбеддингов изображений\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    matrix : list of list of dict\n",
    "        Матрица размера rows * cols, где каждый элемент - словарь с ключами:\n",
    "        - 'id': str\n",
    "        - 'P_click': float (0-1)\n",
    "        - 'P_buy': float (0-1)\n",
    "        - 'P_look1': float (0-1) или None если нет товара сверху\n",
    "        - 'P_look2': float (0-1) или None если нет товара слева/справа\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Генерируем выдачу товаров\n",
    "    results = generate_feed_tfidf(query, n_products, vectorizer, product_vectors, data_df, rows=rows)\n",
    "    \n",
    "    # Вычисляем P_click для каждого товара (TF-IDF схожесть с запросом)\n",
    "    query_processed = preprocess_query(query)\n",
    "    query_vector = vectorizer.transform([query_processed])\n",
    "    \n",
    "    # Получаем индексы товаров в исходном DataFrame\n",
    "    product_indices = [item['vector_index'] for item in results]\n",
    "    product_vectors_subset = product_vectors[product_indices]\n",
    "    \n",
    "    # Вычисляем схожесть TF-IDF\n",
    "    tfidf_similarities = cosine_similarity(query_vector, product_vectors_subset)[0]\n",
    "    # Нормализуем в [0, 1]\n",
    "    min_sim = tfidf_similarities.min()\n",
    "    max_sim = tfidf_similarities.max()\n",
    "    if max_sim - min_sim > 0:\n",
    "        p_click_values = (tfidf_similarities - min_sim) / (max_sim - min_sim)\n",
    "    else:\n",
    "        p_click_values = np.ones_like(tfidf_similarities)\n",
    "    \n",
    "    # Вычисляем P_buy для каждого товара (sentence-transformers схожесть)\n",
    "    if embedding_model is None:\n",
    "        embedding_model = get_embedding_model()\n",
    "    \n",
    "    p_buy_values = []\n",
    "    for item in results:\n",
    "        prob, _ = calculate_purchase_probability(query, item['product_name'], embedding_model=embedding_model)\n",
    "        p_buy_values.append(prob)\n",
    "    \n",
    "    # Вычисляем эмбеддинги изображений для всех товаров\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.applications import VGG16\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.utils import get_file\n",
    "    import os\n",
    "    import tempfile\n",
    "    \n",
    "    if image_model is None:\n",
    "        image_model = VGG16(weights='imagenet', include_top=False)\n",
    "    \n",
    "    def extract_image_embedding(img_url, model):\n",
    "        \"\"\"Извлекает эмбеддинг изображения\"\"\"\n",
    "        try:\n",
    "            if isinstance(img_url, str) and '|' in img_url:\n",
    "                img_url = img_url.split('|')[0]\n",
    "            \n",
    "            if not img_url or pd.isna(img_url):\n",
    "                return np.zeros((512,))\n",
    "            \n",
    "            # Создаем уникальное имя файла\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')\n",
    "            temp_path = temp_file.name\n",
    "            temp_file.close()\n",
    "            \n",
    "            try:\n",
    "                img_path = get_file(temp_path, origin=img_url)\n",
    "                img = image.load_img(img_path, target_size=(224, 224))\n",
    "                img_array = image.img_to_array(img)\n",
    "                img_array = np.expand_dims(img_array, axis=0)\n",
    "                img_array = preprocess_input(img_array)\n",
    "                features = model.predict(img_array, verbose=0)\n",
    "                embedding = features.flatten()\n",
    "                \n",
    "                # Удаляем временный файл\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "                \n",
    "                if np.all(embedding == 0):\n",
    "                    return np.random.rand(512) * 0.01\n",
    "                \n",
    "                return embedding\n",
    "            except Exception as e:\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "                return np.random.rand(512) * 0.01\n",
    "        except:\n",
    "            return np.random.rand(512) * 0.01\n",
    "    \n",
    "    # Извлекаем эмбеддинги для всех товаров\n",
    "    image_embeddings = []\n",
    "    for item in results:\n",
    "        emb = extract_image_embedding(item['image_url'], image_model)\n",
    "        image_embeddings.append(emb)\n",
    "    \n",
    "    image_embeddings = np.array(image_embeddings)\n",
    "    \n",
    "    # Вычисляем матрицу схожести изображений\n",
    "    image_similarity_matrix = cosine_similarity(image_embeddings)\n",
    "    # Нормализуем в [0, 1]\n",
    "    min_sim = image_similarity_matrix.min()\n",
    "    max_sim = image_similarity_matrix.max()\n",
    "    if max_sim - min_sim > 0:\n",
    "        image_similarity_normalized = (image_similarity_matrix - min_sim) / (max_sim - min_sim)\n",
    "    else:\n",
    "        image_similarity_normalized = image_similarity_matrix\n",
    "    \n",
    "    # Формируем матрицу\n",
    "    matrix = []\n",
    "    for row in range(rows):\n",
    "        matrix_row = []\n",
    "        for col in range(cols):\n",
    "            idx = row * cols + col\n",
    "            if idx >= len(results):\n",
    "                matrix_row.append(None)\n",
    "                continue\n",
    "            \n",
    "            item = results[idx]\n",
    "            \n",
    "            # P_look1: схожесть с товаром сверху по изображениям\n",
    "            p_look1 = None\n",
    "            if row > 0:\n",
    "                up_idx = (row - 1) * cols + col\n",
    "                if up_idx < len(results):\n",
    "                    p_look1 = float(image_similarity_normalized[idx, up_idx])\n",
    "            \n",
    "            # P_look2: схожесть с товаром слева или справа по изображениям\n",
    "            p_look2 = None\n",
    "            if col > 0:\n",
    "                # Товар слева\n",
    "                left_idx = row * cols + (col - 1)\n",
    "                if left_idx < len(results):\n",
    "                    p_look2 = float(image_similarity_normalized[idx, left_idx])\n",
    "            elif col < cols - 1:\n",
    "                # Товар справа\n",
    "                right_idx = row * cols + (col + 1)\n",
    "                if right_idx < len(results):\n",
    "                    p_look2 = float(image_similarity_normalized[idx, right_idx])\n",
    "            \n",
    "            matrix_row.append({\n",
    "                'id': item['id'],\n",
    "                'P_click': float(p_click_values[idx]),\n",
    "                'P_buy': float(p_buy_values[idx]),\n",
    "                'P_look1': float(p_look1) if p_look1 is not None else None,\n",
    "                'P_look2': float(p_look2) if p_look2 is not None else None\n",
    "            })\n",
    "        matrix.append(matrix_row)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def display_feed_matrix(matrix, rows=2, cols=2):\n",
    "    \"\"\"\n",
    "    Отображает матрицу выдачи в удобном виде\n",
    "    \n",
    "    Параметры:\n",
    "    -----------\n",
    "    matrix : list of list of dict\n",
    "        Матрица от generate_feed_matrix\n",
    "    rows : int\n",
    "        Количество строк\n",
    "    cols : int\n",
    "        Количество колонок\n",
    "    \"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(\"МАТРИЦА ВЫДАЧИ ТОВАРОВ\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for row in range(rows):\n",
    "        print(f\"\\nСтрока {row + 1}:\")\n",
    "        print(\"-\" * 100)\n",
    "        for col in range(cols):\n",
    "            if matrix[row][col] is None:\n",
    "                print(f\"  Колонка {col + 1}: [пусто]\")\n",
    "                continue\n",
    "            \n",
    "            item = matrix[row][col]\n",
    "            print(f\"\\n  Колонка {col + 1}:\")\n",
    "            print(f\"    ID: {item['id']}\")\n",
    "            print(f\"    P_click (TF-IDF): {item['P_click']:.4f}\")\n",
    "            print(f\"    P_buy (sentence-transformers): {item['P_buy']:.4f}\")\n",
    "            print(f\"    P_look1 (сверху): {item['P_look1']:.4f}\" if item['P_look1'] is not None else \"    P_look1 (сверху): N/A\")\n",
    "            print(f\"    P_look2 (слева/справа): {item['P_look2']:.4f}\" if item['P_look2'] is not None else \"    P_look2 (слева/справа): N/A\")\n",
    "        \n",
    "        if row < rows - 1:\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    \n",
    "    # Сводная таблица\n",
    "    import pandas as pd\n",
    "    flat_data = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            if matrix[row][col] is not None:\n",
    "                item = matrix[row][col].copy()\n",
    "                item['row'] = row + 1\n",
    "                item['col'] = col + 1\n",
    "                flat_data.append(item)\n",
    "    \n",
    "    if flat_data:\n",
    "        df = pd.DataFrame(flat_data)\n",
    "        print(\"\\nСводная таблица:\")\n",
    "        print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования: генерация матрицы выдачи\n",
    "query = \"women cotton dress\"\n",
    "n_products = 4  # 2 строки * 2 колонки\n",
    "rows = 2\n",
    "cols = 2\n",
    "\n",
    "# Генерируем матрицу\n",
    "matrix = generate_feed_matrix(\n",
    "    query=query,\n",
    "    n_products=n_products,\n",
    "    vectorizer=tifd,\n",
    "    product_vectors=tf,\n",
    "    data_df=data,\n",
    "    rows=rows,\n",
    "    cols=cols\n",
    ")\n",
    "\n",
    "# Отображаем результаты\n",
    "display_feed_matrix(matrix, rows=rows, cols=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример: добавление вероятностей покупки к результатам\n",
    "query = \"women cotton dress\"\n",
    "n_products = 4\n",
    "rows = 2\n",
    "\n",
    "# Генерируем выдачу\n",
    "results = generate_feed_tfidf(query, n_products, tifd, tf, data, rows=rows)\n",
    "\n",
    "# Добавляем вероятности покупки (используя готовую модель эмбеддингов)\n",
    "# Функция сама попытается загрузить sentence-transformers, Universal Sentence Encoder или Word2Vec\n",
    "results_with_probs = add_purchase_probabilities_to_results(results, query)\n",
    "print(\"Вероятности покупки вычислены\")\n",
    "\n",
    "# Отображаем результаты с вероятностями покупки\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"РЕЗУЛЬТАТЫ С ВЕРОЯТНОСТЯМИ ПОКУПКИ\")\n",
    "print(\"=\" * 80)\n",
    "for i, item in enumerate(results_with_probs):\n",
    "    print(f\"\\nТовар {i+1}:\")\n",
    "    print(f\"  ID: {item['id']}\")\n",
    "    print(f\"  Название: {item['product_name']}\")\n",
    "    print(f\"  Вероятность покупки: {item.get('purchase_probability', 0.0):.4f}\")\n",
    "    print(f\"  Схожесть с запросом: {item.get('query_similarity', 0.0):.4f}\")\n",
    "    print(f\"  Релевантность: {item.get('relevance_score', 0.0):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:30.676667Z",
     "iopub.status.busy": "2023-12-09T04:16:30.676261Z",
     "iopub.status.idle": "2023-12-09T04:16:30.711620Z",
     "shell.execute_reply": "2023-12-09T04:16:30.710333Z",
     "shell.execute_reply.started": "2023-12-09T04:16:30.676637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41SzDZ4IRdL.jpg|https://images-na.ssl-images-amazon.com/images/I/41bBtt%2BUP7L.jpg|https://images-na.ssl-images-amazon.com/images/I/41SvJR9OukL.jpg|https://images-na.ssl-images-amazon.com/images/I/3189gxVWtdL.jpg|https://images-na.ssl-images-amazon.com/images/I/51vbytfUciL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B0836ZMZC9\n",
      "Brand : PRAKASAM COTTON\n",
      "Product Name : prakasam cotton men kfc plain flexi size welcro pocket dhoti kfc-pln-ash\n",
      "Distance : 0.0\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41qxe1ilCNL.jpg|https://images-na.ssl-images-amazon.com/images/I/31mbXF9uXZL.jpg|https://images-na.ssl-images-amazon.com/images/I/41vmydPg6vL.jpg|https://images-na.ssl-images-amazon.com/images/I/41cwxkRGV4L.jpg|https://images-na.ssl-images-amazon.com/images/I/51OlnayT79L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B01EWZ9WVQ\n",
      "Brand : PRAKASAM COTTON\n",
      "Product Name : prakasam cotton men cotton dhoti\n",
      "Distance : 3.4641016151377544\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41H-jIGK8YL.jpg|https://images-na.ssl-images-amazon.com/images/I/31On22%2BLp%2BL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B00YUJLOQQ\n",
      "Brand : Ramraj\n",
      "Product Name : ramraj men cotton dhoti\n",
      "Distance : 3.605551275463989\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/31vU0-xaYQL.jpg|https://images-na.ssl-images-amazon.com/images/I/31Ek24WaMLL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07C9C6Z7J\n",
      "Brand : PRAKASAM COTTON\n",
      "Product Name : prakasam cotton men winner velcro pocket singl dhoti\n",
      "Distance : 3.605551275463989\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41102rhP15L.jpg|https://images-na.ssl-images-amazon.com/images/I/41aloH7FF3L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07R25DYZN\n",
      "Brand : PRAKASAM COTTON\n",
      "Product Name : prakasam cotton men jari cotton dhoti\n",
      "Distance : 3.605551275463989\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pass the for which product you want similar item and how many wants and the vector \n",
    "# representation of the all the products\n",
    "RecommenderSystems(12381,5,final_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:30.713520Z",
     "iopub.status.busy": "2023-12-09T04:16:30.713191Z",
     "iopub.status.idle": "2023-12-09T04:16:31.126246Z",
     "shell.execute_reply": "2023-12-09T04:16:31.125155Z",
     "shell.execute_reply.started": "2023-12-09T04:16:30.713484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of out text TFIDF vectorizer  (15949, 3082)\n"
     ]
    }
   ],
   "source": [
    "# compute the TF-IDF\n",
    "#tfidf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tifd=TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tf=tifd.fit_transform(data['product_name'])\n",
    "print(\"the shape of out text TFIDF vectorizer \",tf.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Based Recommender systems using Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:31.128211Z",
     "iopub.status.busy": "2023-12-09T04:16:31.127384Z",
     "iopub.status.idle": "2023-12-09T04:16:31.174298Z",
     "shell.execute_reply": "2023-12-09T04:16:31.173217Z",
     "shell.execute_reply.started": "2023-12-09T04:16:31.128179Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41t%2BmC61SFL.jpg|https://images-na.ssl-images-amazon.com/images/I/418A-953osL.jpg|https://images-na.ssl-images-amazon.com/images/I/41RzEpXvUVL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B06WGQLN97\n",
      "Brand : SOFYANA\n",
      "Product Name : sofyana babi gilr birthday parti dress girl pink tissu net 7-8 year\n",
      "Distance : 0.0\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41JPUR9wHSL.jpg|https://images-na.ssl-images-amazon.com/images/I/41OBwX4j0gL.jpg|https://images-na.ssl-images-amazon.com/images/I/51mxvjw3k8L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B078V2W64J\n",
      "Brand : MVD Fashion\n",
      "Product Name : mvd fashion parti dress girl\n",
      "Distance : 0.9323216101199711\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41JEwiwsxoL.jpg|https://images-na.ssl-images-amazon.com/images/I/416zw0LYKQL.jpg|https://images-na.ssl-images-amazon.com/images/I/41bPoFBuqBL.jpg|https://images-na.ssl-images-amazon.com/images/I/41xAR-tYq8L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B071NJXLHJ\n",
      "Brand : ALL ABOUT PINKS\n",
      "Product Name : pink dress girl birthday dress babi girl frock parti dress girl dress girl frock dress frock girl pink & white 2 3 years\n",
      "Distance : 0.9927003226717918\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41JEwiwsxoL.jpg|https://images-na.ssl-images-amazon.com/images/I/416zw0LYKQL.jpg|https://images-na.ssl-images-amazon.com/images/I/41bPoFBuqBL.jpg|https://images-na.ssl-images-amazon.com/images/I/41xAR-tYq8L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B071HC9WKL\n",
      "Brand : ALL ABOUT PINKS\n",
      "Product Name : pink dress girl birthday dress babi girl frock parti dress girl dress girl frock dress frock girl pink & white 6 7 years\n",
      "Distance : 0.9927003226717918\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41Rg9Lbxc3L.jpg|https://images-na.ssl-images-amazon.com/images/I/31AVGYVE1zL.jpg|https://images-na.ssl-images-amazon.com/images/I/41XrSaNtZhL.jpg|https://images-na.ssl-images-amazon.com/images/I/31kK9z5iq%2BL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07Z9GDP82\n",
      "Brand : IAC\n",
      "Product Name : iacjacketkidsb0i\n",
      "Distance : 0.9999999999999998\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41HZEHm2sxL.jpg|https://images-na.ssl-images-amazon.com/images/I/31PTZVRqI6L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07PX9NM64\n",
      "Brand : Sportyway\n",
      "Product Name : sportyway sunris hyderabad ipl jersi\n",
      "Distance : 0.9999999999999998\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/31hBf-7MGeL.jpg|https://images-na.ssl-images-amazon.com/images/I/41bNr7COxcL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B01K6X9QH8\n",
      "Brand : TYR\n",
      "Product Name : tyr vesi femm googl\n",
      "Distance : 0.9999999999999998\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41Nwox6pNNL.jpg|https://images-na.ssl-images-amazon.com/images/I/41WYBXoYyTL.jpg|https://images-na.ssl-images-amazon.com/images/I/41OEN6lXUHL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B01JZ858TU\n",
      "Brand : GO COLORS\n",
      "Product Name : silvergrey-ladieschuridar\n",
      "Distance : 0.9999999999999998\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/51%2Bx2Mh436L.jpg|https://images-na.ssl-images-amazon.com/images/I/41kLs58BP2L.jpg|https://images-na.ssl-images-amazon.com/images/I/41hJQa8hVCL.jpg|https://images-na.ssl-images-amazon.com/images/I/41HgkLFfXJL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B01N6ZKSYM\n",
      "Brand : Etwoa\n",
      "Product Name : etwoa boston terrier blush\n",
      "Distance : 0.9999999999999998\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41Tcjj2ah0L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B00K71YQC0\n",
      "Brand : Bauer\n",
      "Product Name : bauer uncategor\n",
      "Distance : 0.9999999999999998\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41hzK2c%2BYGL.jpg|https://images-na.ssl-images-amazon.com/images/I/41AxHFrP9hL.jpg|https://images-na.ssl-images-amazon.com/images/I/41Y%2Bv3QBMsL.jpg|https://images-na.ssl-images-amazon.com/images/I/61iRZ9%2BLA7L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07H9WKQS7\n",
      "Brand : ADDYVERO\n",
      "Product Name : addyvero girl flounc sleev parti dress\n",
      "Distance : 1.0233659601894667\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/51QObOvbj-L.jpg|https://images-na.ssl-images-amazon.com/images/I/51nHxY74WaL.jpg|https://images-na.ssl-images-amazon.com/images/I/51uXMjRda8L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B06Y28NFH7\n",
      "Brand : ALL ABOUT PINKS\n",
      "Product Name : pink dress girl birthday dress babi girl frock parti dress girl dress girl frock dress frock girl 7-8 year blue\n",
      "Distance : 1.0320000887223209\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/51QObOvbj-L.jpg|https://images-na.ssl-images-amazon.com/images/I/51nHxY74WaL.jpg|https://images-na.ssl-images-amazon.com/images/I/51uXMjRda8L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B06Y28PN6X\n",
      "Brand : ALL ABOUT PINKS\n",
      "Product Name : pink dress girl birthday dress babi girl frock parti dress girl dress girl frock dress frock girl 4-5 year blue\n",
      "Distance : 1.0320000887223209\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41OAPTBbUoL.jpg|https://images-na.ssl-images-amazon.com/images/I/41PLX0FccnL.jpg|https://images-na.ssl-images-amazon.com/images/I/51NgdqcRiML.jpg|https://images-na.ssl-images-amazon.com/images/I/516wRn30UXL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07F8ZLZ7D\n",
      "Brand : ALL ABOUT PINKS\n",
      "Product Name : pink girl pink cotton partywear dress - 2-3 years\n",
      "Distance : 1.039687506158301\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41eHPXWw2wL.jpg|https://images-na.ssl-images-amazon.com/images/I/41BQnulE5zL.jpg|https://images-na.ssl-images-amazon.com/images/I/51eUhwULvoL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07D17KYFS\n",
      "Brand : SOFYANA\n",
      "Product Name : sofyana babi girl tissu net polyest satin birthday dress sfn057peach gown\n",
      "Distance : 1.054467867862887\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pass the for which product you want similar item and how many wants and the vector \n",
    "# representation of the all the products\n",
    "RecommenderSystems(10101,15,tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:16:31.175967Z",
     "iopub.status.busy": "2023-12-09T04:16:31.175676Z",
     "iopub.status.idle": "2023-12-09T04:17:32.424741Z",
     "shell.execute_reply": "2023-12-09T04:17:32.423544Z",
     "shell.execute_reply.started": "2023-12-09T04:16:31.175943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the google model\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_model=KeyedVectors.load_word2vec_format('/kaggle/input/gensimmodel/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T04:17:32.429396Z",
     "iopub.status.busy": "2023-12-09T04:17:32.426238Z",
     "iopub.status.idle": "2023-12-09T07:33:49.162046Z",
     "shell.execute_reply": "2023-12-09T07:33:49.160915Z",
     "shell.execute_reply.started": "2023-12-09T04:17:32.429355Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15949/15949 [3:16:16<00:00,  1.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15949\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each Product Title\n",
    "# words which are present in the w2vec model\n",
    "w2v_words = list(w2v_model.index_to_key)\n",
    "sent_vectors = []; # the avg-w2v for each product title is stored in this list\n",
    "for sent in tqdm(data['product_name']): # for each review/sentence\n",
    "    # length of word vector needed\n",
    "    sent_vec = np.zeros(300) \n",
    "    cnt_words =0; # num of words with a valid vector in the product title\n",
    "    # for each word in title\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T07:33:49.163625Z",
     "iopub.status.busy": "2023-12-09T07:33:49.163284Z",
     "iopub.status.idle": "2023-12-09T07:33:49.717074Z",
     "shell.execute_reply": "2023-12-09T07:33:49.716014Z",
     "shell.execute_reply.started": "2023-12-09T07:33:49.163594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# store the vector for product title vector in the pikle file so no need to run the above\n",
    "# code again to generate it\n",
    "import pickle\n",
    "file_path = '/kaggle/working/product.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(sent_vectors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T07:44:56.174436Z",
     "iopub.status.busy": "2023-12-09T07:44:56.174006Z",
     "iopub.status.idle": "2023-12-09T07:44:56.512007Z",
     "shell.execute_reply": "2023-12-09T07:44:56.510907Z",
     "shell.execute_reply.started": "2023-12-09T07:44:56.174402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read the pickle file again\n",
    "file_path = '/kaggle/working/product.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    product_vector = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T07:44:59.428432Z",
     "iopub.status.busy": "2023-12-09T07:44:59.428000Z",
     "iopub.status.idle": "2023-12-09T07:44:59.461924Z",
     "shell.execute_reply": "2023-12-09T07:44:59.460730Z",
     "shell.execute_reply.started": "2023-12-09T07:44:59.428399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pv = np.array(product_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remmendation using the word2Vec for Text to vector and ecludian distance as similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T07:45:18.821748Z",
     "iopub.status.busy": "2023-12-09T07:45:18.821320Z",
     "iopub.status.idle": "2023-12-09T07:45:18.879703Z",
     "shell.execute_reply": "2023-12-09T07:45:18.878694Z",
     "shell.execute_reply.started": "2023-12-09T07:45:18.821711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41ZgWt6cVcL.jpg|https://images-na.ssl-images-amazon.com/images/I/41ZgWt6cVcL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B003PKPKES\n",
      "Brand : Chevrolet\n",
      "Product Name : chevrolet corvett c6 flex black basebal cap\n",
      "Distance : 0.0\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41R8p%2Bb7tYL.jpg|https://images-na.ssl-images-amazon.com/images/I/41R8p%2Bb7tYL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B003PKVT5M\n",
      "Brand : Chevrolet\n",
      "Product Name : corvett c6 flex red basebal cap\n",
      "Distance : 0.1990861492919667\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41SBt-q2bWL.jpg|https://images-na.ssl-images-amazon.com/images/I/31qFCGbWy5L.jpg|https://images-na.ssl-images-amazon.com/images/I/413SfLjmdSL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B0154REIJW\n",
      "Brand : Verceys\n",
      "Product Name : vercey black white casual belt women - pack 2\n",
      "Distance : 0.2772062629528931\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41MJi5Z195L.jpg|https://images-na.ssl-images-amazon.com/images/I/41lT9ojAyGL.jpg|https://images-na.ssl-images-amazon.com/images/I/311rzpte7HL.jpg|https://images-na.ssl-images-amazon.com/images/I/31sUbX%2BlaYL.jpg|https://images-na.ssl-images-amazon.com/images/I/41Xp7J5gCnL.jpg|https://images-na.ssl-images-amazon.com/images/I/317yoXAUiVL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B019ZJ1WWG\n",
      "Brand : Black Collection\n",
      "Product Name : black collect solid men v-neck strecthabl shirt\n",
      "Distance : 0.317829593468222\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41N5WdU9XbL.jpg|https://images-na.ssl-images-amazon.com/images/I/41UrUCot7UL.jpg|https://images-na.ssl-images-amazon.com/images/I/41jfL4WSeBL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07T6KYG37\n",
      "Brand : Calvus\n",
      "Product Name : calvus men black windcheat hidden collar pocket cap\n",
      "Distance : 0.3241353139908091\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RecommenderSystems(4310,5,pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighthed TF-IDF word2vec Based Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T07:45:22.521202Z",
     "iopub.status.busy": "2023-12-09T07:45:22.520821Z",
     "iopub.status.idle": "2023-12-09T08:32:10.251162Z",
     "shell.execute_reply": "2023-12-09T08:32:10.250289Z",
     "shell.execute_reply.started": "2023-12-09T07:45:22.521173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15949/15949 [46:47<00:00,  5.68it/s] \n"
     ]
    }
   ],
   "source": [
    "tifd=TfidfVectorizer()\n",
    "tifd.fit_transform(data['product_name'])\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(tifd.get_feature_names_out(), list(tifd.idf_)))\n",
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tifd.get_feature_names_out() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each product is stored in this list\n",
    "row=0;\n",
    "for sent in tqdm(data['product_name']): # for each products\n",
    "    sent_vec = np.zeros(300) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the products\n",
    "    for word in sent.split(): # for each word in a product\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model[word]\n",
    "#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T08:32:10.253601Z",
     "iopub.status.busy": "2023-12-09T08:32:10.252781Z",
     "iopub.status.idle": "2023-12-09T08:32:10.423945Z",
     "shell.execute_reply": "2023-12-09T08:32:10.423045Z",
     "shell.execute_reply.started": "2023-12-09T08:32:10.253567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# store the vector for product title vector in the pikle file so no need to run the above\n",
    "# code again to generate it\n",
    "file_path = '/kaggle/working/product_tf.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(tfidf_sent_vectors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T08:32:10.426088Z",
     "iopub.status.busy": "2023-12-09T08:32:10.425175Z",
     "iopub.status.idle": "2023-12-09T08:32:10.500551Z",
     "shell.execute_reply": "2023-12-09T08:32:10.499356Z",
     "shell.execute_reply.started": "2023-12-09T08:32:10.426054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41kSuAymBlL.jpg|https://images-na.ssl-images-amazon.com/images/I/410IR9WpMzL.jpg|https://images-na.ssl-images-amazon.com/images/I/41eirEVtkBL.jpg|https://images-na.ssl-images-amazon.com/images/I/41zwJumwhKL.jpg|https://images-na.ssl-images-amazon.com/images/I/41u6cieYcZL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07KT61N7D\n",
      "Brand : JAIPUR ATTIRE\n",
      "Product Name : jaipur attir women cotton flare kurti\n",
      "Distance : 0.0\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41dztihNJeL.jpg|https://images-na.ssl-images-amazon.com/images/I/41pOc5-BgvL.jpg|https://images-na.ssl-images-amazon.com/images/I/41TvZGgryvL.jpg|https://images-na.ssl-images-amazon.com/images/I/61-4skD5OrL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07W2Z1JRV\n",
      "Brand : VEDANA\n",
      "Product Name : vedana cotton print flare kurti pink\n",
      "Distance : 0.9250530634480277\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41VC1KFeHfL.jpg|https://images-na.ssl-images-amazon.com/images/I/41TsQGXdB0L.jpg|https://images-na.ssl-images-amazon.com/images/I/413ziKcZXzL.jpg|https://images-na.ssl-images-amazon.com/images/I/41IuJtyF2CL.jpg|https://images-na.ssl-images-amazon.com/images/I/41Dt6QeEFaL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07WGQTBP9\n",
      "Brand : R.R.TEXTILES\n",
      "Product Name : rrtextil red flare rayon kurti women design kurti girl\n",
      "Distance : 0.9532673267324706\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41crwTJKvPL.jpg|https://images-na.ssl-images-amazon.com/images/I/41FmCgHYNvL.jpg|https://images-na.ssl-images-amazon.com/images/I/41l1En-A-EL.jpg|https://images-na.ssl-images-amazon.com/images/I/51KKG6FYkYL.jpg|https://images-na.ssl-images-amazon.com/images/I/41UhuzRhhKL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07MH5BWDY\n",
      "Brand : PALSIYA PRESENTS\n",
      "Product Name : palsiya present women rayon navyblu flare print kurti\n",
      "Distance : 0.9555220972030847\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/31ECeYZ0gTL.jpg|https://images-na.ssl-images-amazon.com/images/I/31zk7EuPvHL.jpg|https://images-na.ssl-images-amazon.com/images/I/31Ff-3K4bzL.jpg|https://images-na.ssl-images-amazon.com/images/I/31yx4AyKKkL.jpg|https://images-na.ssl-images-amazon.com/images/I/51g3IJXMr0L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B0765ZLGF1\n",
      "Brand : JAIPUR ATTIRE\n",
      "Product Name : jaipur attir women cotton slub straight kurti\n",
      "Distance : 0.9572684858360978\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RecommenderSystems(100,5,tfidf_sent_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also Add the Brand based Recommendation along with the title of product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T08:32:10.503933Z",
     "iopub.status.busy": "2023-12-09T08:32:10.503102Z",
     "iopub.status.idle": "2023-12-09T08:32:11.170894Z",
     "shell.execute_reply": "2023-12-09T08:32:11.169775Z",
     "shell.execute_reply.started": "2023-12-09T08:32:10.503887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  as we number of unique brands are 5403 we can use the One Hot Encoding to Encode \n",
    "# all these brand\n",
    "# Encode all the brand\n",
    "brand_vec = pd.get_dummies(data['brand'], prefix='brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T09:04:47.206877Z",
     "iopub.status.busy": "2023-12-09T09:04:47.206437Z",
     "iopub.status.idle": "2023-12-09T09:04:47.215906Z",
     "shell.execute_reply": "2023-12-09T09:04:47.214965Z",
     "shell.execute_reply.started": "2023-12-09T09:04:47.206842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Now We have brand and title both to compute or find the recommendation \n",
    "# most often we need biased based Recommendation \n",
    "# change the model with adding the bias as well\n",
    "def weightedRecommenderSystems(p_id,numProduct,vectors,category_weight):\n",
    "    \"\"\"\n",
    "    p_id is the index from data that  is preprocessed\n",
    "    numProduct of wanted to recommeded\n",
    "    category on what feature concat to create\n",
    "    category weights what weight of these category can be\n",
    "    vectors contains set of vector for each of category based on their weight\n",
    "    \"\"\"\n",
    "    # create an weighted Vectors\n",
    "    for i in range(len(vectors)):\n",
    "        vectors[i]=np.array(vectors[i])*category_weight[i]\n",
    "    vectors=np.concatenate(vectors, axis=1)\n",
    "    # Compute the Pairwise Distances\n",
    "    dist=pairwise_distances(vectors,vectors[p_id].reshape(1,-1))\n",
    "    # sort them based the distance computed\n",
    "    indices=np.argsort(dist.flatten())[0:numProduct]\n",
    "    # this will store distances between the product_titles\n",
    "    p_dist=np.sort(dist.flatten())[0:numProduct]\n",
    "    # take product details for these indices\n",
    "    similar_product=list(data.index[indices])\n",
    "    for i in range(len(indices)):\n",
    "        # generate the heatmap as well\n",
    "        display(Image(url=data['medium'].loc[similar_product[i]],width=100, height=100))\n",
    "        print('ASIN :',data['asin'].loc[similar_product[i]])\n",
    "        print('Brand :',data['brand'].loc[similar_product[i]])\n",
    "        print('Product Name :',data['product_name'].loc[similar_product[i]])\n",
    "        print('Distance :',p_dist[i])\n",
    "        print('_'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T09:06:53.081898Z",
     "iopub.status.busy": "2023-12-09T09:06:53.081380Z",
     "iopub.status.idle": "2023-12-09T09:06:54.024836Z",
     "shell.execute_reply": "2023-12-09T09:06:54.023651Z",
     "shell.execute_reply.started": "2023-12-09T09:06:53.081862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/31PHV5YFpQL.jpg|https://images-na.ssl-images-amazon.com/images/I/31m5y37YvhL.jpg|https://images-na.ssl-images-amazon.com/images/I/31BQ4utpjYL.jpg|https://images-na.ssl-images-amazon.com/images/I/319ROLdVP1L.jpg|https://images-na.ssl-images-amazon.com/images/I/41Gjk5QlqGL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07SJZKS6Z\n",
      "Brand : Varanga\n",
      "Product Name : varanga women cotton print a-lin handloom dress off white  xxl\n",
      "Distance : 0.0\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41%2Bqmzus8YL.jpg|https://images-na.ssl-images-amazon.com/images/I/41LqmkLzT-L.jpg|https://images-na.ssl-images-amazon.com/images/I/4149PjjZ95L.jpg|https://images-na.ssl-images-amazon.com/images/I/61fOgWSWmJL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07GXYQ7K7\n",
      "Brand : RLB Fashion\n",
      "Product Name : rlb fashion women cotton silk handloom dhakai jamdani sare rlb-00547 pink free size\n",
      "Distance : 0.9966957731430856\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41YUUb-jDWL.jpg|https://images-na.ssl-images-amazon.com/images/I/41BTNJdPY9L.jpg|https://images-na.ssl-images-amazon.com/images/I/41JYe9bPXeL.jpg|https://images-na.ssl-images-amazon.com/images/I/61%2Bouq0-GWL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07RKJ8K3K\n",
      "Brand : RLB Fashion\n",
      "Product Name : rlb fashion women cotton silk handloom dhakai jamdani sare white red\n",
      "Distance : 1.011840379029101\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41zCXViZV3L.jpg|https://images-na.ssl-images-amazon.com/images/I/51KKBnOE%2BSL.jpg|https://images-na.ssl-images-amazon.com/images/I/51zq4QlVLcL.jpg|https://images-na.ssl-images-amazon.com/images/I/61ibergRonL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07BVCNS9C\n",
      "Brand : RLB Fashion\n",
      "Product Name : rlb fashion women cotton silk handloom dhakai jamdani sare rlb-00435 grey free size\n",
      "Distance : 1.0298298867020217\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41binlSDOXL.jpg|https://images-na.ssl-images-amazon.com/images/I/41Fm5ES3buL.jpg|https://images-na.ssl-images-amazon.com/images/I/31FI7kWp62L.jpg|https://images-na.ssl-images-amazon.com/images/I/316TFcFL1AL.jpg|https://images-na.ssl-images-amazon.com/images/I/51LOTLEn8iL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07YWNP586\n",
      "Brand : Genric\n",
      "Product Name : women chandana ikkat pure handloom pochamp ikat cotton unstitch dress materi red free size\n",
      "Distance : 1.0569849069345518\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/413OsN1glML.jpg|https://images-na.ssl-images-amazon.com/images/I/41SceLGeZ%2BL.jpg|https://images-na.ssl-images-amazon.com/images/I/31pucpFtbpL.jpg|https://images-na.ssl-images-amazon.com/images/I/21qWeDxXt6L.jpg|https://images-na.ssl-images-amazon.com/images/I/61djiygWMUL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07YWN727W\n",
      "Brand : Genric\n",
      "Product Name : women chandana ikkat handloom pochamp ikat cotton unstitch dress materi blue free size\n",
      "Distance : 1.0626977478026942\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/51mXC3MeYUL.jpg|https://images-na.ssl-images-amazon.com/images/I/417r8EjA-9L.jpg|https://images-na.ssl-images-amazon.com/images/I/61ioPAljGAL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07TMPFWF9\n",
      "Brand : Handloom Palace\n",
      "Product Name : handloom palac hand block cotton floral print veget color run dress materi unstitch kantha fabric\n",
      "Distance : 1.0973368616899726\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41fiHP1U6CL.jpg|https://images-na.ssl-images-amazon.com/images/I/41QV8T8sfhL.jpg|https://images-na.ssl-images-amazon.com/images/I/41zTlZZ6ZNL.jpg|https://images-na.ssl-images-amazon.com/images/I/41jnTBRQycL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07V4F6PBF\n",
      "Brand : T.J. SAREES\n",
      "Product Name : tj sare women handloom cotton silk hand print sare black & red\n",
      "Distance : 1.1070599712548617\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41850Zy3%2BpL.jpg|https://images-na.ssl-images-amazon.com/images/I/41izFI0zJAL.jpg|https://images-na.ssl-images-amazon.com/images/I/41wQjf2qf2L.jpg|https://images-na.ssl-images-amazon.com/images/I/31awbv%2B6ckL.jpg|https://images-na.ssl-images-amazon.com/images/I/41QFaNtsEWL.jpg|https://images-na.ssl-images-amazon.com/images/I/31NOYEg2knL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07Q5XXFMG\n",
      "Brand : WoodenTant\n",
      "Product Name : woodent red cotton silk soft dhakai jamdani handloom sare\n",
      "Distance : 1.140453280100687\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/311S8j3wAfL.jpg|https://images-na.ssl-images-amazon.com/images/I/310JhB3WgAL.jpg|https://images-na.ssl-images-amazon.com/images/I/31VQwjjIGjL.jpg|https://images-na.ssl-images-amazon.com/images/I/31yspa9HRVL.jpg|https://images-na.ssl-images-amazon.com/images/I/31qtIWVVBqL.jpg|https://images-na.ssl-images-amazon.com/images/I/51MNuAjathL.jpg|https://images-na.ssl-images-amazon.com/images/I/31HJ-eNu7nL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07FGG9475\n",
      "Brand : QueenShield\n",
      "Product Name : queenshield women casual & formal handloom cotton kurti plus small size â€¦\n",
      "Distance : 1.1404963390440899\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# giving more weights to the title then brands \n",
    "# based on weights and category vector passed will return the Recommended Products\n",
    "weightedRecommenderSystems(2215,10,[tfidf_sent_vectors,brand_vec],[1,.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Using the Image as item based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-12-09T12:57:16.151042Z",
     "iopub.status.busy": "2023-12-09T12:57:16.150672Z",
     "iopub.status.idle": "2023-12-09T12:57:17.212866Z",
     "shell.execute_reply": "2023-12-09T12:57:17.211760Z",
     "shell.execute_reply.started": "2023-12-09T12:57:16.151012Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 483ms/step\n",
      "[[[[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   ...\n",
      "   [9.2670784e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    3.6524075e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    4.3967037e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      "  [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   ...\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    4.7062035e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    6.8217209e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    8.4057932e+00 0.0000000e+00]]\n",
      "\n",
      "  [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   ...\n",
      "   [1.7189224e+01 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    6.1034271e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    1.0161579e+01 0.0000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    3.4375233e+01 0.0000000e+00]\n",
      "   [1.2337188e+01 0.0000000e+00 5.7301430e+01 ... 2.3036079e+00\n",
      "    4.1875515e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 2.2888744e+01 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   ...\n",
      "   [7.6748085e-01 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    2.0209306e-01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      "  [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    6.1753757e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 1.0932808e+01 ... 2.5223146e+01\n",
      "    9.5454880e+01 0.0000000e+00]\n",
      "   [6.2788963e-02 0.0000000e+00 3.9282639e+01 ... 1.6548719e+01\n",
      "    4.8616405e+01 0.0000000e+00]\n",
      "   ...\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    3.4867549e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    6.5169902e+00 0.0000000e+00]]\n",
      "\n",
      "  [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    1.7924366e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    5.2833878e+01 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.5363168e+01\n",
      "    4.9857952e+01 0.0000000e+00]\n",
      "   ...\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.3250934e+01\n",
      "    0.0000000e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    6.3448710e+00 0.0000000e+00]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "    0.0000000e+00 0.0000000e+00]]]]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# step1 : Extract the Feature from image using an pretrained model of Vgg16\n",
    "# Extracting feature from images\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.utils import get_file\n",
    "import numpy as np\n",
    "# Using the Tensorflow to extract freatue\n",
    "# using the pretrained model of VGG16 which is an cnn for image dataset \n",
    "# extracting by removing the last layer of model\n",
    "\n",
    "#step1.1 Loading the pre-trained VGG16 model\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "def extract_features(img_path):\n",
    "    # download the image store it temerarly\n",
    "    img_path = get_file('image.jpg', origin=img_path)\n",
    "    #  resizing all images in 244*244 size\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert the image to array\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # preprocess the array as the input required for the vgg16 model\n",
    "    # using the function preprocess_input\n",
    "    img_array = preprocess_input(img_array)\n",
    "    features = model.predict(img_array)\n",
    "    # return the feature extracted\n",
    "    return features.flatten()\n",
    "\n",
    "# for each image find its feature extract from above funtion and store them as a list\n",
    "img_features = data['medium'].apply(extract_features)\n",
    "print(len(img_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation using the Image as a input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T13:06:38.711011Z",
     "iopub.status.busy": "2023-12-09T13:06:38.710587Z",
     "iopub.status.idle": "2023-12-09T13:06:43.120192Z",
     "shell.execute_reply": "2023-12-09T13:06:43.118948Z",
     "shell.execute_reply.started": "2023-12-09T13:06:38.710979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# store the extracted feature vector in pickle file\n",
    "file_path = '/kaggle/working/product_img.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(img_features, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T13:11:30.426677Z",
     "iopub.status.busy": "2023-12-09T13:11:30.426266Z",
     "iopub.status.idle": "2023-12-09T13:11:34.376744Z",
     "shell.execute_reply": "2023-12-09T13:11:34.375548Z",
     "shell.execute_reply.started": "2023-12-09T13:11:30.426644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41ZgWt6cVcL.jpg|https://images-na.ssl-images-amazon.com/images/I/41ZgWt6cVcL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B003PKPKES\n",
      "Brand : Chevrolet\n",
      "Product Name : chevrolet corvett c6 flex black basebal cap\n",
      "Distance : 3.0517578125e-05\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41R8p%2Bb7tYL.jpg|https://images-na.ssl-images-amazon.com/images/I/41R8p%2Bb7tYL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B003PKVT5M\n",
      "Brand : Chevrolet\n",
      "Product Name : corvett c6 flex red basebal cap\n",
      "Distance : 0.5220768925258924\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/51Pzi-JpENL.jpg|https://images-na.ssl-images-amazon.com/images/I/41K7YBBDCBL.jpg|https://images-na.ssl-images-amazon.com/images/I/41tM2mQxICL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B0147NMIL4\n",
      "Brand : Chevrolet\n",
      "Product Name : hot rod plus chevrolet corvett c7 logo gray basebal hat\n",
      "Distance : 1.140350975759019\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41BoIr1T0zL.jpg|https://images-na.ssl-images-amazon.com/images/I/41vc6ocf-JL.jpg|https://images-na.ssl-images-amazon.com/images/I/41cguGuq4eL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B003MRDHNK\n",
      "Brand : Chevrolet\n",
      "Product Name : chevrolet camaro ralli stripe yellow basebal cap\n",
      "Distance : 1.187063581454872\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41IAwFMm5sL.jpg|https://images-na.ssl-images-amazon.com/images/I/318OZb6bFsL.jpg|https://images-na.ssl-images-amazon.com/images/I/31yjKH0mz6L.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B004X8XHZ0\n",
      "Brand : Chevrolet\n",
      "Product Name : corvett c3 cross flag black basebal cap\n",
      "Distance : 1.3958643297026474\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/31lNQA0U1pL.jpg|https://images-na.ssl-images-amazon.com/images/I/31lNQA0U1pL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B003FPA6MY\n",
      "Brand : Chevrolet\n",
      "Product Name : camaro z28 liquid metal basebal hat\n",
      "Distance : 1.4469381076354135\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41qB0SvUUuL.jpg|https://images-na.ssl-images-amazon.com/images/I/41IRDZ-zj7L.jpg|https://images-na.ssl-images-amazon.com/images/I/41o8j2IRiSL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B01MUHMHZH\n",
      "Brand : Chevrolet\n",
      "Product Name : c7 corvett stingray hat - black embroid gestur logo\n",
      "Distance : 1.7522674212709526\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/51gWsXHKuOL.jpg|https://images-na.ssl-images-amazon.com/images/I/51RJvWN6hyL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B0010ZVYPO\n",
      "Brand : High-End Motorsports\n",
      "Product Name : high-end motorsport c6 cottontwil black hat compat 2005-2013 chevrolet corvett\n",
      "Distance : 1.7986300956911845\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/31jhGsxxfwL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B00KKXLKUQ\n",
      "Brand : Joe's USA\n",
      "Product Name : premium flex fit hat - high perform cool & dri basebal cap 7 color\n",
      "Distance : 1.8146561207771337\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://images-na.ssl-images-amazon.com/images/I/41hNi4XqgJL.jpg|https://images-na.ssl-images-amazon.com/images/I/41kSsnFlXML.jpg|https://images-na.ssl-images-amazon.com/images/I/41tVVB%2BsUxL.jpg|https://images-na.ssl-images-amazon.com/images/I/31VvTlME8pL.jpg|https://images-na.ssl-images-amazon.com/images/I/41A7VqvRiWL.jpg\" width=\"100\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN : B07T1B89LV\n",
      "Brand : Alcove\n",
      "Product Name : alcov unisex black red navyblu white ny half meshhalf net basebal fabric cotton cap combo 4 curv visor freesizeadjust\n",
      "Distance : 1.8199569236930293\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## we can generate as many as product we want as we can compare with different \n",
    "# model by having the all brand,image,title in considration the model \n",
    "# recommends perfectly\n",
    "weightedRecommenderSystems(4310,10,[tfidf_sent_vectors,brand_vec,features],[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1187537,
     "sourceId": 1986395,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4129140,
     "sourceId": 7151478,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 1461,
     "sourceId": 1727,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
